{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "Name: S. Kaan Cetindag\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T=[]\n",
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "\n",
    "with open('transitions.csv','rb') as t:\n",
    "    reader=csv.reader(t,delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "T=np.array(T)\n",
    "T=T.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "For the first question, I created a cumulative sum of the distribution and drawed from that distribution with an uniform random selection $ \\mathcal{U}[0,1] $. The chosen character is the character with the nearest larger probability value to our drawing. The process is repeated by changing the distribution with the previously chosen characters distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ainatithithel.theesonour\n"
     ]
    }
   ],
   "source": [
    "def findNearMin(array, val):\n",
    "    diff=array-val                #difference between each element and given value\n",
    "    positive=np.where(diff>=0)    #get the index of >0 elements in diff array\n",
    "    return min(positive[0])       #return the min index that is larger than 0\n",
    "\n",
    "def generateString(first_letter,N):\n",
    "    x_prev=first_letter\n",
    "    string=x_prev\n",
    "\n",
    "    for i in range(N):\n",
    "        x_prevRow=np.cumsum(np.asarray(T[letter2idx[x_prev]],dtype=float)) #distribution of the given char as a cum. sum\n",
    "        w=np.random.rand()                                                 #uniform random variable\n",
    "        randPick=findNearMin(x_prevRow,w)                                  #get the larger nearest index to the random variable\n",
    "        x_prev = alphabet[randPick]                                        #append the letter in that index to the string\n",
    "        string+=x_prev\n",
    "    print string\n",
    "    \n",
    "generateString('.',24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "To generate the missing letters we use both the previous and next given letters surrounding the missing letters. Even though our model is a Markov(1) model, the use of following given letters is necessary. A simple analogy can be finding a route, knowing the starting point and destination. The process can be formulated as:\n",
    "\n",
    "For a sequence of $n$ letters, we have 2 letters that encapusulate the string and $n-2$ letters that are unknown.\n",
    "\n",
    "\\begin{equation*}\n",
    "p(x_{begin+1},x_{end-1}|x_{begin},x_{end})\\propto p(x_{end}|x_{end-1})p(x_{end-1}|x_{end-2})....p(x_{begin+2|x_{begin+1}})p(x_{begin+1}|x_{begin})\n",
    "\\end{equation*}\n",
    "\n",
    "The above equation can be solved by having $T^{n-1}$, getting the row of the end letter distribution and multiplicating it with our beginning letter distribution. The final distribution can be normalized and drawed from similar to question 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th__br__n.f_x.\n",
      "the.brvin.fex.\n",
      "\n",
      "_u_st__n_.to_be._nsw_r__\n",
      "oussthand.toube.answar.h\n",
      "\n",
      "i__at_._a_h_n_._e_r_i_g\n",
      "it.at..hathend.fedraing\n",
      "\n",
      "q___t.___z._____t.__.___.__.\n",
      "qur.t.mprz.pomy.t.te.fig.fr.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateStringGiven(givenString):\n",
    "    unknowns=np.empty(0)\n",
    "    knowns=np.empty(0)\n",
    "    diffs=np.empty(0)\n",
    "    prevs=np.empty(0)\n",
    "    nexts=np.empty(0)\n",
    "    \n",
    "    for idx,ch in enumerate(givenString):\n",
    "        if ch=='_':\n",
    "            unknowns=np.append(unknowns,idx)\n",
    "    \n",
    "    def pickChar(dist):\n",
    "        dist=np.cumsum(dist, dtype=float)\n",
    "        w=np.random.rand()\n",
    "        pick=findNearMin(dist,w)\n",
    "        return alphabet[pick]\n",
    "    \n",
    "    def getPrev(val):\n",
    "        diff=knowns-val\n",
    "        negative=np.where(diff<=0)[0]\n",
    "        if negative.shape[0]==0:\n",
    "            return None\n",
    "        else:        \n",
    "            return int(knowns[max(negative)])\n",
    "        \n",
    "    def getNext(val):\n",
    "        diff=knowns-val\n",
    "        positive=np.where(diff>=0)[0]\n",
    "        if positive.shape[0]==0:\n",
    "            return None\n",
    "        else:        \n",
    "            return int(knowns[min(positive)])                              \n",
    "     \n",
    "    def generate(previdx,nextidx):\n",
    "        if previdx==None and nextidx!=None:\n",
    "            n=1\n",
    "            prevLetter='.'\n",
    "            nextLetter=givenString[nextidx]\n",
    "        elif nextidx==None and previdx!=None:\n",
    "            n=len(givenString)-previdx-1\n",
    "            nextLetter='.'\n",
    "            prevLetter=givenString[previdx]\n",
    "        else:\n",
    "            n=nextidx-previdx-1\n",
    "            prevLetter=givenString[previdx]\n",
    "            nextLetter=givenString[nextidx]\n",
    "        \n",
    "        prev_dist=T[letter2idx[prevLetter]]\n",
    "        next_dist=np.linalg.matrix_power(T,n).transpose()[letter2idx[nextLetter]]\n",
    "        final_dist=prev_dist*next_dist\n",
    "        final_dist/=np.sum(final_dist)\n",
    "        return pickChar(final_dist)\n",
    "    \n",
    "    for j in unknowns:\n",
    "        j=int(j)\n",
    "        for idx,ch in enumerate(givenString):\n",
    "            if ch=='_':\n",
    "                unknowns=np.append(unknowns,idx)\n",
    "            else:\n",
    "                knowns=np.append(knowns,idx)\n",
    "\n",
    "        prevGiven=getPrev(j)\n",
    "        nextGiven=getNext(j)\n",
    "        \n",
    "        addChar=generate(prevGiven,nextGiven)\n",
    "        s=list(givenString)\n",
    "        s[j]=addChar\n",
    "        givenString=''.join(s)\n",
    "    print givenString\n",
    "        \n",
    "        \n",
    "for k in range(len(test_strings)):\n",
    "    print test_strings[k]\n",
    "    generateStringGiven(test_strings[k])\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Similar to question 2, but we thake the *argmax()* of the final distribution achieved by matrix power operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th__br__n.f_x.\n",
      "the.br.an.fex.\n",
      "\n",
      "_u_st__n_.to_be._nsw_r__\n",
      "oursthend.to.be.answered\n",
      "\n",
      "i__at_._a_h_n_._e_r_i_g\n",
      "in.ath.wathend.he.r.ing\n",
      "\n",
      "q___t.___z._____t.__.___.__.\n",
      "qur.t.thiz.the.at.an.the.an.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generateStringGiven(givenString):\n",
    "    unknowns=np.empty(0)\n",
    "    knowns=np.empty(0)\n",
    "    diffs=np.empty(0)\n",
    "    prevs=np.empty(0)\n",
    "    nexts=np.empty(0)\n",
    "    \n",
    "    for idx,ch in enumerate(givenString):\n",
    "        if ch=='_':\n",
    "            unknowns=np.append(unknowns,idx)\n",
    "    \n",
    "    def pickChar(dist):\n",
    "        dist=np.cumsum(dist, dtype=float)\n",
    "        w=np.random.rand()\n",
    "        pick=findNearMin(dist,w)\n",
    "        return alphabet[pick]\n",
    "    \n",
    "    def getPrev(val):\n",
    "        diff=knowns-val\n",
    "        negative=np.where(diff<=0)[0]\n",
    "        if negative.shape[0]==0:\n",
    "            return None\n",
    "        else:        \n",
    "            return int(knowns[max(negative)])\n",
    "        \n",
    "    def getNext(val):\n",
    "        diff=knowns-val\n",
    "        positive=np.where(diff>=0)[0]\n",
    "        if positive.shape[0]==0:\n",
    "            return None\n",
    "        else:        \n",
    "            return int(knowns[min(positive)])                              \n",
    "     \n",
    "    def generate(previdx,nextidx):\n",
    "        if previdx==None and nextidx!=None:\n",
    "            n=1\n",
    "            prevLetter='.'\n",
    "            nextLetter=givenString[nextidx]\n",
    "        elif nextidx==None and previdx!=None:\n",
    "            n=len(givenString)-previdx-1\n",
    "            nextLetter='.'\n",
    "            prevLetter=givenString[previdx]\n",
    "        else:\n",
    "            n=nextidx-previdx-1\n",
    "            prevLetter=givenString[previdx]\n",
    "            nextLetter=givenString[nextidx]\n",
    "        \n",
    "        prev_dist=T[letter2idx[prevLetter]]\n",
    "        next_dist=np.linalg.matrix_power(T,n).transpose()[letter2idx[nextLetter]]\n",
    "        final_dist=prev_dist*next_dist\n",
    "        final_dist/=np.sum(final_dist)\n",
    "        return alphabet[np.argmax(final_dist)]\n",
    "    \n",
    "    for j in unknowns:\n",
    "        j=int(j)\n",
    "        for idx,ch in enumerate(givenString):\n",
    "            if ch=='_':\n",
    "                unknowns=np.append(unknowns,idx)\n",
    "            else:\n",
    "                knowns=np.append(knowns,idx)\n",
    "\n",
    "        prevGiven=getPrev(j)\n",
    "        nextGiven=getNext(j)\n",
    "        \n",
    "        addChar=generate(prevGiven,nextGiven)\n",
    "        s=list(givenString)\n",
    "        s[j]=str(addChar)\n",
    "        givenString=''.join(s)\n",
    "    print givenString\n",
    "        \n",
    "        \n",
    "for k in range(len(test_strings)):\n",
    "    print test_strings[k]\n",
    "    generateStringGiven(test_strings[k])\n",
    "    print \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Models such as Markov(2) and higher would be more suitable for better estimations because dependency to only the first preceeding letter is a poor modeling of languages. This would increase the complexity of the estimation process but in turn also  increase the correct estimation. \n",
    "\n",
    "Another alternative could be using well defined $\\phi(x_1,x_2)$ functions to implement the forward-backward algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
