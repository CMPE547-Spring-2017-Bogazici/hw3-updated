{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we model the English words as a Markov chain of letters and try to generate random words and also try to estimate the missing letters of a given word. The model is somewhat cruel estimation however results do not seem to be reasonable considering the structure of English words.\n",
    "\n",
    "The following code snippet loads the probability matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "\n",
    "The following code generates random english words with a sepecified length using the probability matrix. To increase efficiency we utilize Markov property. Since the letters are only dependent on the previous letter, the letters can be generated iteratively starting from the beginning. Formally,\n",
    "$$x_i\\sim P(x_i|x_{i-1})$$ \n",
    "where $x_1\\sim p(x_1|\\text{'.'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample : 1     ppr.o\n",
      "Sample : 2     strl.t\n",
      "Sample : 3     ro.fry.\n",
      "Sample : 4     jucer.ta\n",
      "Sample : 5     aayd.f.pp\n",
      "Sample : 6     wine.bre.d\n",
      "Sample : 7     t....be.fom\n",
      "Sample : 8     te.to.an.o.w\n",
      "Sample : 9     he.sed.intha.\n",
      "Sample : 10     serne.ther.thi\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "#Number of letters\n",
    "def rand_word(N):\n",
    "    letters=[]\n",
    "    #Assume that every words starts with a '.'\n",
    "    letters.append('.');\n",
    "    #Iteratively generate letters.\n",
    "    for i in range (0,N):\n",
    "        prev=letters[i]\n",
    "        prev_id=letter2idx[prev]\n",
    "        column=T[prev_id]\n",
    "        probs=np.array([float(j) for j in column])\n",
    "        probs /= probs.sum()\n",
    "        c=np.random.choice(alphabet,p=probs)\n",
    "        letters.append(c)\n",
    "    str=\"\"\n",
    "    for j in letters[1:]:\n",
    "        str+=j\n",
    "    return str\n",
    "N=15\n",
    "for i in range (5,N):\n",
    "    print(\"Sample :\",i-4,\"   \",rand_word(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK2\n",
    "\n",
    "The task is to fill the missing letters in given English words. Without implementing more general sum-product algorithm on the factor graph, we can efficiently estimate the words by using the Markov(1) property. This implementation is in fact a very special case of general sum-product algorithm. \n",
    "\n",
    "First thing to notice is that seperate chunk of missing letters are independent from each other. For instance for the word $\\text{'th__br__n.f_x.'}$ we can divide this word as a set of words $\\text{S={'h__b','r__n','f_x'}}$ and fill the missing letters of each word seperately. This comes from the fact that whenever a letter is given, the chain is broken from that point.\n",
    "\n",
    "Estimation is performed recursively starting from the last element of the missing chunk. \n",
    "\n",
    "Let's say that we have a sequence of $x_2=a,x_3,x_4,x_5,x_6=t$,\n",
    "\n",
    "\n",
    "To estimate  $x_5$ we should marginalize the probability distribution by summing over other missing letters. Formally,\n",
    "\n",
    "$$P(x_5|x_2=a,x_6=t)\\propto \\sum_{x_3,x_4} P(x_3,x_4,x_5|x_2=a,x_6=t)$$\n",
    "$$P(x_5|x_2=a,x_6=t)\\propto \\sum_{x_3,x_4} P(x_3|x_2=a)P(x_4|x3)P(x_5|x4)P(x_6=t|x_5)$$\n",
    "\n",
    "Reorganizing terms,\n",
    "\n",
    "$$P(x_5|x_2=a,x_6=t)\\propto \\sum_{x_3,x_4} P(x_6=t|x_5)P(x_5|x4)P(x_4|x3)P(x_3|x_2=a)$$\n",
    "\n",
    "This is mathematically equivalent to,\n",
    "\n",
    "$$P(x_5|x_2=a,x_6=t)\\propto P(x_6=t|x_5)\\sum_{x_4}P(x_5|x4)\\sum_{x_3}P(x_4|x3)P(x_3|x_2=a)$$\n",
    "\n",
    "Let's call $\\vec{u}=p(x_6=t|x_5)$ and $\\vec{v^T}=p(x_3|x_2=a)$\n",
    "\n",
    "Notice that these sums correspond to matrix product. \n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$$P(x_5|x_2=a,x_6=t)\\propto (\\vec{u}T^2)\\odot \\vec{v^T} $$ \n",
    "\n",
    "where $\\odot$ denotes element-wise product and T is probability matrix. (Due to Markov property, as we go forward the previous given terms becomes unnecessary )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  th__br__n.f_x.\n",
      "Sample  1 :  the.bryon.fex.\n",
      "Sample  2 :  thi.brirn.fix.\n",
      "Sample  3 :  thanbrman.fex.\n",
      "Sample  4 :  the.brean.fex.\n",
      "Sample  5 :  theabrven.fex.\n",
      "Sample  6 :  thoubry.n.fox.\n",
      "Sample  7 :  the.broun.fox.\n",
      "Sample  8 :  tho.br.an.fox.\n",
      "Sample  9 :  thd.brean.fex.\n",
      "Sample  10 :  the.brinn.f.x.\n",
      "Word:  _u_st__n_.to_be._nsw_r__\n",
      "Sample  1 :  qutsthono.to.be.answuren\n",
      "Sample  2 :  puesthane.tombe.answarie\n",
      "Sample  3 :  tuestren..toube.answorye\n",
      "Sample  4 :  culste.nd.toube.answernl\n",
      "Sample  5 :  ougst.ind.tombe.answarrr\n",
      "Sample  6 :  cuosthang.to.be.onswar.p\n",
      "Sample  7 :  mutst.ony.to.be.onsware.\n",
      "Sample  8 :  jussthing.to.be.answor.d\n",
      "Sample  9 :  qutstheng.tombe..nswirit\n",
      "Sample  10 :  ourste.nd.tombe.answor.l\n",
      "Word:  i__at_._a_h_n_._e_r_i_g\n",
      "Sample  1 :  in.ate.pathind.peer.ing\n",
      "Sample  2 :  ie.ato.lachint.wetr.ing\n",
      "Sample  3 :  ingath.fatheng..e.rfing\n",
      "Sample  4 :  ispate..athand.re.rming\n",
      "Sample  5 :  it.ath.bathano.bepr.ing\n",
      "Sample  6 :  ifeatw.fanhend.beerming\n",
      "Sample  7 :  ithate.waghang.pearling\n",
      "Sample  8 :  idiate.hachand.he.r.ing\n",
      "Sample  9 :  iceati.sashind.seer.ing\n",
      "Sample  10 :  i.hath.wathand.be.r.ing\n",
      "Word:  q___t.___z._____t.__.___.__.\n",
      "Sample  1 :  qurat.r.tz.hissat.at.w.w.ai.\n",
      "Sample  2 :  quiat.on.z.eorent.ve.s.h.gy.\n",
      "Sample  3 :  qul.t.morz..ang.t.te.ome.at.\n",
      "Sample  4 :  quist.ourz.te.mat.ny.cte.hy.\n",
      "Sample  5 :  qutst.cooz..ancat.id.ind.be.\n",
      "Sample  6 :  qur.t.rirz.an.d.t..e.two.ad.\n",
      "Sample  7 :  qul.t.menz.beeg.t..d.mbe.we.\n",
      "Sample  8 :  quret.as.z.s.k.at.th.d.h.al.\n",
      "Sample  9 :  qut.t.ocaz.emetlt.nd.ted.to.\n",
      "Sample  10 :  qutit.morz.tig..t.ss.bio.in.\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "#Convert T to float array.\n",
    "T=np.array(T).astype(np.float)\n",
    "#Estimate the letters in a given interval.\n",
    "def estimate_interval(low,high,previ,nexti,charList):\n",
    "        length=high-low+1\n",
    "        if length==0:\n",
    "            return charList\n",
    "        \n",
    "        row=T[:][previ]\n",
    "        if(high==len(charList)-1):\n",
    "            column=1\n",
    "        else :\n",
    "            column=[t[nexti] for t in T]\n",
    "        \n",
    "        v=row\n",
    "        if length>1:\n",
    "            v=np.dot(v,LA.matrix_power(T, length-1))\n",
    "        \n",
    "        w=list(v*column)\n",
    "        final=np.random.choice(alphabet,p=w/np.sum(w))\n",
    "        charList[high]=final\n",
    "        if length==1:\n",
    "            return charList\n",
    "        else:\n",
    "            return estimate_interval(low,high-1,previ,letter2idx[final],charList)\n",
    "#Estimate the missing letters in a given string.        \n",
    "def fill_missing_letters(string):    \n",
    "    N=len(string)\n",
    "    unknown=[]\n",
    "    sum_terms=[]\n",
    "    for ch in range (0,N):\n",
    "        current=string[ch]\n",
    "        if current=='?' or current=='_':\n",
    "            unknown.append(ch)\n",
    "\n",
    "\n",
    "    for i in range(0,len(unknown)):\n",
    "        temp=[]\n",
    "        if i==0:\n",
    "            temp.append(unknown[i])\n",
    "            sum_terms.append(temp)\n",
    "        elif unknown[i]-unknown[i-1]==1:\n",
    "            sum_terms[len(sum_terms)-1].append(unknown[i])\n",
    "        else :\n",
    "            temp.append(unknown[i])\n",
    "            sum_terms.append(temp)\n",
    "\n",
    "\n",
    "        \n",
    "    result=list(list(string))\n",
    "    for i in range(0,len(sum_terms)):\n",
    "        low=sum_terms[i][0]\n",
    "        up=sum_terms[i][len(sum_terms[i])-1]\n",
    "        prevId=letter2idx[string[low-1]]\n",
    "        if(up!=len(string)-1):\n",
    "            nextId=letter2idx[string[up+1]]\n",
    "        else :\n",
    "            nextId=-1\n",
    "\n",
    "        result=estimate_interval(low,up,prevId,nextId,result)\n",
    "\n",
    "    return result\n",
    "\n",
    "for i in test_strings:\n",
    "    print(\"Word: \",i)\n",
    "    for j in range (0,10):\n",
    "        result='.'+i\n",
    "        result=fill_missing_letters(result)\n",
    "        print(\"Sample \",j+1,\": \",''.join(result[1:len(result)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK3\n",
    "\n",
    "We can use the same algorithm with little modification. For each missing element, we can choose the letter with highest probability. I started from the last missing element of a chunk which can affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the.br.an.fex. -4.43532766488\n",
      "oursthand.to.be.answere. -15.7481002491\n",
      "in.ath.wathend.he.r.ing -16.7873293326\n",
      "qus.t.herz.athe.t.he.the.he. -31.1291155683\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "#Convert T to float array.\n",
    "T=np.array(T).astype(np.float)\n",
    "\n",
    "def estimate_interval(low,high,previ,nexti,charList):\n",
    "        length=high-low+1\n",
    "        if length==0:\n",
    "            return charList,1\n",
    "        \n",
    "        row=T[previ]\n",
    "        if(high==len(charList)-1):\n",
    "            column=1\n",
    "        else :\n",
    "            column=[t[nexti] for t in T]\n",
    "        \n",
    "        v=row\n",
    "        if length>1:\n",
    "            v=np.dot(v,LA.matrix_power(T, length-1))\n",
    "        \n",
    "        w=list(v*column)\n",
    "        w=list(w/np.sum(w))\n",
    "        final=w.index(np.max(w))\n",
    "        charList[high]=alphabet[final]\n",
    "        if length==1:\n",
    "            return charList,np.max(w)\n",
    "        else:\n",
    "            tempList,prob=estimate_interval(low,high-1,previ,letter2idx[alphabet[final]],charList)\n",
    "            return tempList,prob*np.max(w)\n",
    " \n",
    "\n",
    "def fill_missing_letters(string):    \n",
    "    N=len(string)\n",
    "    unknown=[]\n",
    "    sum_terms=[]\n",
    "    for ch in range (0,N):\n",
    "        current=string[ch]\n",
    "        if current=='?' or current=='_':\n",
    "            unknown.append(ch)\n",
    "\n",
    "\n",
    "    for i in range(0,len(unknown)):\n",
    "        temp=[]\n",
    "        if i==0:\n",
    "            temp.append(unknown[i])\n",
    "            sum_terms.append(temp)\n",
    "        elif unknown[i]-unknown[i-1]==1:\n",
    "            sum_terms[len(sum_terms)-1].append(unknown[i])\n",
    "        else :\n",
    "            temp.append(unknown[i])\n",
    "            sum_terms.append(temp)\n",
    "\n",
    "\n",
    "        \n",
    "    result=list(list(string))\n",
    "    logprob=1\n",
    "    for i in range(0,len(sum_terms)):\n",
    "        low=sum_terms[i][0]\n",
    "        up=sum_terms[i][len(sum_terms[i])-1]\n",
    "        prevId=letter2idx[string[low-1]]\n",
    "        if(up!=len(string)-1):\n",
    "            nextId=letter2idx[string[up+1]]\n",
    "        else :\n",
    "            nextId=-1\n",
    "\n",
    "        result,prob=estimate_interval(low,up,prevId,nextId,result)\n",
    "        logprob*=prob\n",
    "\n",
    "    return result,logprob\n",
    "\n",
    "for i in test_strings:\n",
    "    result='.'+i\n",
    "    result,logprob=fill_missing_letters(result)\n",
    "    print(''.join(result[1:len(result)]),np.log2(logprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality letters are not dependent on only one previous letter, but two or more letters. To take into account this dependency, we may use Markov(n) model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
