{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Variables created here is used later on in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "alphabet = list(string.ascii_lowercase)\n",
    "alphabet.append('.')\n",
    "transition = {}\n",
    "for indx1, prob in enumerate(alphabet):\n",
    "    letter = {}\n",
    "    for indx2, given in enumerate(alphabet):\n",
    "        letter[given] = float(T[indx1][indx2])\n",
    "    transition[prob] = letter\n",
    "# transition['q']['u'] = 0.9949749 -> p(x_t = 'u' | x_{t-1} = 'q')\n",
    "Tt = np.array([np.array(list(prob.values())) for prob in list(transition.values())])\n",
    "# Tt[16] -> p(x_t| x_{t-1} = 'q')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Generator\n",
    "\n",
    "Implements only forward with given conditional probabilities generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ins.atichooponthe.thisorind.se.na\n"
     ]
    }
   ],
   "source": [
    "def generate_string(transition, N):\n",
    "    alphabet = list(transition.keys())\n",
    "    # initial character is always '.'\n",
    "    string = \".\"\n",
    "    for i in range(1, N + 1):\n",
    "        letter_probs = np.array(list(transition[string[i-1]].values()))\n",
    "        letter_probs_normalized = np.true_divide(letter_probs, np.sum(letter_probs))\n",
    "        string += np.random.choice(alphabet, p=letter_probs_normalized)[0]\n",
    "    return string[1:]\n",
    "print(generate_string(transition, np.random.randint(1, 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward-Backward Generator\n",
    "\n",
    "Utilizes ending character for generation by implementing forward-backward algorithm, expected to create better creations. Most likely implementation is wrong so does not seem to give better results. Also does not uses log-probabilities in calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th__br__n.f_x.\n",
      "th.rbr.on.fax.\n",
      "_u_st__n_.to_be._nsw_r__\n",
      "euest.ono.to.be.ensw.re.\n",
      "i__at_._a_h_n_._e_r_i_g\n",
      "ieeati.oaihanl.eeer.iag\n",
      "q___t.___z._____t.__.___.__.\n",
      "qoeat.ee.z.eeeeat.ee.ee..ee.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def f_b(transition, sub_str):\n",
    "    '''\n",
    "    Forward-Backward algorithm for Markov-1\n",
    "    '''\n",
    "    alphabet = list(transition.keys())\n",
    "    # propagate probability using initial condition(known letter)\n",
    "    # prior is 1 and not considered since first letter is known\n",
    "    f = [Tt[letter2idx[sub_str[0]]]]\n",
    "    for i in range(len(sub_str) -2):\n",
    "        f.append(np.multiply(f[i], Tt))\n",
    "    for i in reversed(range(len(sub_str) -2)):\n",
    "        # choose vector from probability matrix for known second letter\n",
    "        probs = f[i+1][letter2idx[sub_str[i+2]]]\n",
    "        # normalize probability vector\n",
    "        probs = np.true_divide(probs, np.sum(probs))\n",
    "        # sample a letter from alphabet with this probability\n",
    "        sub_str = sub_str[:i+1] + np.random.choice(alphabet, p=probs)[0] + sub_str[i+2:]\n",
    "    return sub_str\n",
    "\n",
    "def complete_string(t, given_string):\n",
    "    # add spaces to beginning and to the end\n",
    "    given_string = '.' + given_string + '.'\n",
    "    # find intervals to be filled\n",
    "    sub_strs = [[match.start()-1, match.end()+1] for match in re.compile('_+').finditer(given_string)]\n",
    "    # fill intervals by using generator one by one\n",
    "    for r in sub_strs:\n",
    "        given_string = given_string[:r[0]] + f_b(t, given_string[r[0]:r[1]]) + given_string[r[1]:]\n",
    "    # return only filled string\n",
    "    return given_string[1:-1]\n",
    "\n",
    "for test_string in test_strings:\n",
    "    print(test_string)\n",
    "    print(complete_string(transition, test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most-Likely Generator\n",
    "\n",
    "Changes prediction part with argmax function which should give the most likely string and it is fixed since there is no polling. Then again there most likely  something is something wrong with implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th__br__n.f_x.\n",
      "thhnbr.on.f.x.\n",
      "_u_st__n_.to_be._nsw_r__\n",
      "euestuone.to.be.ensw.r..\n",
      "i__at_._a_h_n_._e_r_i_g\n",
      "ieeat..eaehhne.eeer.ieg\n",
      "q___t.___z._____t.__.___.__.\n",
      "qeeat.ee.z..ttttt.ee.eee.ee.\n"
     ]
    }
   ],
   "source": [
    "# Same implementation with above, only difference is\n",
    "# using argmax in sampling instead of polling from distribution.\n",
    "def viterbi(transition, sub_str):\n",
    "    alphabet = list(transition.keys())\n",
    "    f = [Tt[letter2idx[sub_str[0]]]]\n",
    "    for i in range(len(sub_str) -2):\n",
    "        f.append(np.multiply(f[i], Tt))\n",
    "    for i in reversed(range(len(sub_str) -2)):\n",
    "        probs = f[i+1][letter2idx[sub_str[i+2]]]\n",
    "        probs = np.true_divide(probs, np.sum(probs))\n",
    "        sub_str = sub_str[:i+1] + alphabet[np.argmax(probs)] + sub_str[i+2:]\n",
    "    return sub_str\n",
    "\n",
    "def complete_string_mostlikely(t, given_string):\n",
    "    given_string = '.' + given_string + '.'\n",
    "    sub_strs = [[match.start()-1, match.end()+1] for match in re.compile('_+').finditer(given_string)]\n",
    "    for r in sub_strs:\n",
    "        given_string = given_string[:r[0]] + viterbi(t, given_string[r[0]:r[1]]) + given_string[r[1]:]\n",
    "    return given_string[1:-1]\n",
    "\n",
    "for test_string in test_strings:\n",
    "    print(test_string)\n",
    "    print(complete_string_mostlikely(transition, test_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Generator\n",
    "\n",
    "Using higher level Markov model should give better results for these applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
