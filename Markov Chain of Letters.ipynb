{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Mehmetcan TÜTÜNCÜ\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Homework 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'t'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '0.0393295')\n",
      "('b', '0.0001590')\n",
      "('c', '0.0037195')\n",
      "('d', '0.0000674')\n",
      "('e', '0.0892434')\n",
      "('f', '0.0009218')\n",
      "('g', '0.0000404')\n",
      "('h', '0.3352928')\n",
      "('i', '0.0666758')\n",
      "('j', '0.0000054')\n",
      "('k', '0.0000162')\n",
      "('l', '0.0146273')\n",
      "('m', '0.0009110')\n",
      "('n', '0.0011051')\n",
      "('o', '0.0913053')\n",
      "('p', '0.0000809')\n",
      "('q', '0.0000027')\n",
      "('r', '0.0310281')\n",
      "('s', '0.0245378')\n",
      "('t', '0.0171177')\n",
      "('u', '0.0185732')\n",
      "('v', '0.0000027')\n",
      "('w', '0.0078702')\n",
      "('x', '0.0000000')\n",
      "('y', '0.0121422')\n",
      "('z', '0.0002776')\n",
      "('.', '0.2449470')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'t'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['t']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1\n",
    "For the Markov(1) model, $p( x_1,x_2,...,x_N | x_0 ) = p(x_1|x_0)p(x_2|x_1) ... p(x_n|x_{n-1})$\n",
    "\n",
    "We can utilize numpy.random.choice function to generate variates from discrete distributions.\n",
    "\n",
    "(Probability vectors are normalized to sum up 1 in this example since they have precisional errors causing sums of 1.00001 or 0.99999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "t.\n",
      "tow\n",
      "myir\n",
      "h.cot\n",
      "bacly.\n",
      "it.itib\n",
      "es.tone.\n",
      "ge.ot.far\n",
      ".gmy.ichra\n",
      "susosplyiom\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tempMatrix= np.asarray(T)\n",
    "probMatrix= np.zeros(shape=(27,27))\n",
    "for letter in alphabet:\n",
    "    probMatrix[letter2idx[letter]] = (tempMatrix[letter2idx[letter]].astype(np.float))/(sum(tempMatrix[letter2idx[letter]].astype(np.float)))\n",
    "#probMatrix[letter2idx['q']]   #-> vector of probabilities, Pr(X_(i+1)|X_i='q')\n",
    "#print np.sum(probMatrix[letter2idx['a']])\n",
    "#needed to normalize so they sum up to 1. Some of them are 1.0001 or 0.9999\n",
    "def guessNextLetter(previousLetter):\n",
    "    return np.random.choice(alphabet,p= probMatrix[letter2idx[previousLetter]])\n",
    "print guessNextLetter('t')    \n",
    "   \n",
    "def genRandomString(N):\n",
    "    randomString = '.'\n",
    "    randomString+= guessNextLetter(randomString[-1])\n",
    "    for i in range(1,N):\n",
    "        randomString+= guessNextLetter(randomString[-1])\n",
    "    return randomString\n",
    "#generates 10 random strings\n",
    "for i in range(2,12):\n",
    "    print(genRandomString(i)[1:])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "For a missing letter in the string, only variables that matter for the distribution is the  first known letter that comes before and the first letter that comes after the missing one under Markov(1) assumption.\n",
    "Define a substring $X_{0:(N+1)}$ with the length of $N+2$ where $x_0$ and $x_{N+1}$ are known and $x_{0:N}$ are missing.\n",
    "$$p(x_{1:N}{\\mid}x_0=\\hat{x}_0,x_{N+1}=\\hat{x}_{N+1}) \\propto p(x_{N+1}=\\hat{x}_{N+1}{\\mid}x_{1:N},x_0=\\hat{x}_0)  p(x_N{\\mid}x_{1:N-1},x_0=\\hat{x}_0)p(x_{N-1}{\\mid}x_{1:N-2},x_0=\\hat{x}_0)...p(x_1{\\mid}x_0=\\hat{x}_0)$$\n",
    "equivalently,\n",
    "$$p(x_{1:N}{\\mid}x_0=\\hat{x}_0,x_{N+1}=\\hat{x}_{N+1}) \\propto p(x_{N+1}=\\hat{x}_{N+1}{\\mid}x_N) p(x_N{\\mid}x_{N-1})p(x_{N-1}{\\mid}x_{N-2})...p(x_2|x_1)p(x_1|x_0=\\hat{x}_0)$$\n",
    "\n",
    "$\\begin{equation}\\label{eq:1}\n",
    "p(x_{1}{\\mid}x_0=\\hat{x}_0,x_{N+1}=\\hat{x}_{N+1}) \\propto p(x_N=\\hat{x}_N \\mid x_1) p(x_1 \\mid x_0=\\hat{x}_0)\n",
    "\\end{equation}$\n",
    "In the expression above, $p(x_N=\\hat{x}_N \\mid x_1)$ is a column in $N$'th power of the transition matrix( $T^N$).\n",
    "For the solution strategy, program will iterate over the string, find the longest consecutive missing streak at the beginning of the string. After finding the longest possible missing substring, it will change a character according to the distribution, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th__br__n.f_x.\n",
      "Trial  0 :  the.brsin.fix.\n",
      "Trial  1 :  the.brgen.fex.\n",
      "Trial  2 :  the.br.an.fex.\n",
      "Trial  3 :  the.bruan.fex.\n",
      "Trial  4 :  the.br.hn.fax.\n",
      "Trial  5 :  the.bri.n.fux.\n",
      "Trial  6 :  the.brean.fex.\n",
      "Trial  7 :  tha.br.un.fex.\n",
      "Trial  8 :  the.brorn.fex.\n",
      "Trial  9 :  the.br.in.fex.\n",
      "_u_st__n_.to_be._nsw_r__\n",
      "Trial  0 :  ourstcono.to.be.answarle\n",
      "Trial  1 :  bussteind.tombe.inswhri.\n",
      "Trial  2 :  bunsthind.to.be.answorou\n",
      "Trial  3 :  tursthand.toybe.answarer\n",
      "Trial  4 :  cutsthong.to.be.inswir.y\n",
      "Trial  5 :  buesthend.toube.inswbrud\n",
      "Trial  6 :  wurst.ine.to.be.inswor.s\n",
      "Trial  7 :  mutsth.nd.toube.answeres\n",
      "Trial  8 :  burstoong.torbe.answer.t\n",
      "Trial  9 :  tuisthend.to.be.onswired\n",
      "i__at_._a_h_n_._e_r_i_g\n",
      "Trial  0 :  illato.vathon..he.r.irg\n",
      "Trial  1 :  ithato.fathine.pe.rhing\n",
      "Trial  2 :  ithati.ta.hens.tear.ing\n",
      "Trial  3 :  is.ath.fashend.me.r.ing\n",
      "Trial  4 :  ingate.cathund.herrsing\n",
      "Trial  5 :  igsatw.wathind.wearking\n",
      "Trial  6 :  i.hato..athind.bepruing\n",
      "Trial  7 :  incate.tachend.berrging\n",
      "Trial  8 :  is.ati.waghind.serruing\n",
      "Trial  9 :  insaty..awhend.teereing\n",
      "q___t.___z._____t.__.___.__.\n",
      "Trial  0 :  qumat.a.rz.fimyot.ee.wor.rr.\n",
      "Trial  1 :  qut.t.inoz.ot.wit.th.ala.gl.\n",
      "Trial  2 :  qurst.chiz.wnthet.pr.dis.pe.\n",
      "Trial  3 :  qut.t.e.rz.sere.t..d.sos.at.\n",
      "Trial  4 :  qulat.herz..ned.t.ay.hey.ce.\n",
      "Trial  5 :  qun.t.imoz.m.wont.an.ie..lt.\n",
      "Trial  6 :  qusat.spez.my.y.t.s..ig..pe.\n",
      "Trial  7 :  qus.t.h.az.bre.st.nd.gld.be.\n",
      "Trial  8 :  qufot.f.iz.he.f.t.hy.t.s.io.\n",
      "Trial  9 :  quiot.k.iz.thy..t..d.tin.re.\n"
     ]
    }
   ],
   "source": [
    "def probBetweenLetters(letterFirst,letterLast,countMissing=1):\n",
    "    xFirst= letter2idx[letterFirst]\n",
    "    xLast= letter2idx[letterLast]\n",
    "    #calculation of the probability vector\n",
    "    probabilityVector=np.linalg.matrix_power(probMatrix,countMissing)[:,xLast] * probMatrix[xFirst]\n",
    "    #normalization of the vector\n",
    "    probabilityVector /= sum(probabilityVector)\n",
    "    return probabilityVector\n",
    "def fillMissing(string):\n",
    "    string= '.'+string+'.'\n",
    "    \n",
    "    missingIndices = [i for i, x in enumerate(list(string)) if x in ['_','?']]\n",
    "    \n",
    "    if not missingIndices:\n",
    "        return string[1:][:-1]\n",
    "    \n",
    "    #detects the first consecutive missing streak and randoms the first letter\n",
    "    firstMissingIndex=missingIndices[0]\n",
    "    i=0\n",
    "    if(len(missingIndices) > 1):\n",
    "        while( (missingIndices[i+1]-missingIndices[i])==1 ):\n",
    "            i=i+1\n",
    "            if((i+1)==len(missingIndices)):\n",
    "                break    \n",
    "        \n",
    "        lastMissingIndex = firstMissingIndex+ i\n",
    "        guess= np.random.choice(alphabet,p=probBetweenLetters(string[firstMissingIndex-1],string[lastMissingIndex+1],\n",
    "                                             lastMissingIndex-firstMissingIndex+1))\n",
    "    else:\n",
    "        guess=np.random.choice(alphabet,p=probBetweenLetters(string[firstMissingIndex-1],string[firstMissingIndex+1],1))\n",
    "        \n",
    "    #remove dot's from the beginning and the end,  replace predicted letter with the first missing value detected\n",
    "    changedString=string[1:firstMissingIndex]+guess+ string[(firstMissingIndex+1):-1]\n",
    "    \n",
    "    #recursive solution on each recursion, we change simply one letter \n",
    "    return(fillMissing(changedString))\n",
    "    \n",
    "for string in test_strings:\n",
    "    print string\n",
    "    for i in range(10):\n",
    "        print \"Trial \",i,\": \",fillMissing(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "A similar idea is used to evaluate best possible canditates. Evaluating argmax at every step for each $x_i$, will guarantee the highest probability possible. Since $p(x_i)$ here are all positive and selected iteratively $max(\\prod{x_i}) = \\prod{max(p_i}$) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing String: th__br__n.f_x. Log Probability\n",
      "Filled String : the.br.an.fex. -3.07433488138\n",
      "Missing String: _u_st__n_.to_be._nsw_r__ Log Probability\n",
      "Filled String : oursthend.to.be.answered -11.0693283596\n",
      "Missing String: i__at_._a_h_n_._e_r_i_g Log Probability\n",
      "Filled String : in.ath.wathend.he.r.ing -11.6360900332\n",
      "Missing String: q___t.___z._____t.__.___.__. Log Probability\n",
      "Filled String : qur.t.thiz.the.at.an.the.an. -22.9236427638\n"
     ]
    }
   ],
   "source": [
    "def BestFillMissing(string,logProbability=0):\n",
    "    string= '.'+string+'.'\n",
    "    \n",
    "    missingIndices = [i for i, x in enumerate(list(string)) if x in ['_','?']]\n",
    "    \n",
    "    if not missingIndices:\n",
    "        return (string[1:][:-1],logProbability)\n",
    "    \n",
    "    #detects the first consecutive missing streak and randoms the first letter\n",
    "    firstMissingIndex=missingIndices[0]\n",
    "    i=0\n",
    "    if(len(missingIndices) > 1):\n",
    "        while( (missingIndices[i+1]-missingIndices[i])==1 ):\n",
    "            i=i+1\n",
    "            if((i+1)==len(missingIndices)):\n",
    "                break    \n",
    "        \n",
    "        lastMissingIndex = firstMissingIndex+ i\n",
    "        probVector= probBetweenLetters(string[firstMissingIndex-1],string[lastMissingIndex+1],\n",
    "                                             lastMissingIndex-firstMissingIndex+1)\n",
    "        guess=np.argmax(probVector) #instead of generating a random variate, we return the maximum element of probVector\n",
    "        logProbabilityNew= logProbability+  np.log(probVector[guess])\n",
    "        guess=letter2idx.keys()[letter2idx.values().index(guess)]\n",
    "    else:\n",
    "        probVector = probBetweenLetters(string[firstMissingIndex-1],string[firstMissingIndex+1],1)\n",
    "        guess=np.argmax(probVector)\n",
    "        logProbabilityNew= logProbability+  np.log(probVector[guess])\n",
    "        guess=letter2idx.keys()[letter2idx.values().index(guess)]\n",
    "        \n",
    "    #remove dot's from the beginning and the end,  replace predicted letter with the first missing value detected\n",
    "    changedString=string[1:firstMissingIndex]+guess+ string[(firstMissingIndex+1):-1]\n",
    "    #recursive solution on each recursion, we change simply one letter, and accumulate the logProbability value\n",
    "    return(BestFillMissing(changedString,logProbabilityNew))    \n",
    "\n",
    "for string in test_strings:\n",
    "    filledString,probability= BestFillMissing(string)\n",
    "    print \"Missing String:\",string, \"Log Probability\"\n",
    "    print \"Filled String :\",filledString, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "This version of model can only detect relationships within the distance of 1. Deeper Markov models will perform better than current model. Instead of feeding letters one by one , we could also predict the words that are completely missing with a holistic approach using a pre-created English dictionary to make predictions by Markov(1) model with a big transition matrix storing $(N_{Words})^2$ probabilities (it might exceed memory limitations if there is any)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
