{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Åžirag Erkol\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '0.0002835')\n",
      "('b', '0.0228302')\n",
      "('c', '0.0369041')\n",
      "('d', '0.0426290')\n",
      "('e', '0.0012216')\n",
      "('f', '0.0075739')\n",
      "('g', '0.0171385')\n",
      "('h', '0.0014659')\n",
      "('i', '0.0372661')\n",
      "('j', '0.0002353')\n",
      "('k', '0.0110124')\n",
      "('l', '0.0778259')\n",
      "('m', '0.0260757')\n",
      "('n', '0.2145354')\n",
      "('o', '0.0005459')\n",
      "('p', '0.0195213')\n",
      "('q', '0.0001749')\n",
      "('r', '0.1104770')\n",
      "('s', '0.0934290')\n",
      "('t', '0.1317960')\n",
      "('u', '0.0098029')\n",
      "('v', '0.0306574')\n",
      "('w', '0.0088799')\n",
      "('x', '0.0009562')\n",
      "('y', '0.0233701')\n",
      "('z', '0.0018701')\n",
      "('.', '0.0715219')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(T)):\n",
    "    T[i]=list(map(float,T[i]))\n",
    "for i in range(len(T)):\n",
    "    T[i]=T[i]/np.sum(T[i])\n",
    "T=np.vstack(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1(N):\n",
    "    text='.'\n",
    "    for i in range(N):\n",
    "        letter=np.random.choice(alphabet,1,p=T[alphabet.index(text[i])])\n",
    "        text=text+letter.tostring()\n",
    "    print text[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef.in.bof.manirees.atheser.thas.ce.vie.theele.has.s.ant.mes.hespo.lllte.achitlo.iatred..adeve.as.n.\n"
     ]
    }
   ],
   "source": [
    "f1(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the probability values, we first need to check for the conditional dependencies. Assume we have a string $'a__df_c'$. Let's first number the empty letters: 'a23df6c'. Since we are using a Markov(1) model, it can be said that the empty letters are only dependent on the given last previous letter and the first next letter. In the case of empty cell 2, these letters are 'a' and 'd'. Then, the probability distribution turns out to be as following:\n",
    "$$p(2|1,4)=\\frac{p(1,2,4)}{p(1,4)}=\\frac{p(4|2) \\times p(2|1) \\times p(1)}{p(4|1) \\times p(1)}=\\frac{p(4|2) \\times p(2|1)}{p(4|1)}=\\frac{p(4='d'|2) \\times p(2|1='a')}{p(4='d'|1='a')}$$<br>\n",
    "This calculation will give the necessary probability distribution for the second cell, when the cell 1 and 4 are given. The following two parts are done using these calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f2(text):\n",
    "    text='.'+text\n",
    "    textnew=''\n",
    "    lastLetter=9999\n",
    "    if text[len(text)-1]==\"_\":\n",
    "        for i in range(len(text)):\n",
    "            if text[i]!=\"_\":\n",
    "                lastLetter=i\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '_' :\n",
    "            matrix1=T\n",
    "            matrix2=T\n",
    "            matrix3=T\n",
    "            if i > lastLetter:\n",
    "                prob=matrix1[alphabet.index(textnew[i-1])]\n",
    "            else:\n",
    "                for j in range(len(text)):\n",
    "                    if j > i and text[j]!='_':\n",
    "                        ind2=j\n",
    "                        break\n",
    "                if ind2 - i > 1:\n",
    "                    for k in range(ind2 - i - 1):\n",
    "                        matrix2=np.dot(matrix2,T)\n",
    "                for k in range(ind2 - (i - 1) - 1):\n",
    "                    matrix3=np.dot(matrix3,T)\n",
    "                prob=np.multiply(matrix1[alphabet.index(textnew[i-1])],matrix2[:,alphabet.index(text[ind2])])/matrix3[alphabet.index(textnew[i-1]),alphabet.index(text[ind2])]\n",
    "            letter=np.random.choice(alphabet,1,p=prob)\n",
    "            textnew=textnew+letter.tostring()\n",
    "        else:\n",
    "            textnew=textnew+text[i]\n",
    "    return textnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1 - Sample 0: .the.brlin.fex.\n",
      "Word 1 - Sample 1: .the.br.an.fex.\n",
      "Word 1 - Sample 2: .tho.br.in.fex.\n",
      "Word 1 - Sample 3: .the.br.an.fex.\n",
      "Word 1 - Sample 4: .thi.br.en.fex.\n",
      "Word 1 - Sample 5: .thimbr.on.fix.\n",
      "Word 1 - Sample 6: .theabrern.fex.\n",
      "Word 1 - Sample 7: .theabr.on.f.x.\n",
      "Word 1 - Sample 8: .tha.br.in.fex.\n",
      "Word 1 - Sample 9: .tht.br.on.fex.\n",
      "\n",
      "Word 2 - Sample 0: .tunstrint.tombe.onsweri.\n",
      "Word 2 - Sample 1: .ou.stwong.tofbe.answerom\n",
      "Word 2 - Sample 2: .ounstotny.torbe.onswire.\n",
      "Word 2 - Sample 3: .mutstsend.to.be.inswhrec\n",
      "Word 2 - Sample 4: .outsthins.tosbe.answaran\n",
      "Word 2 - Sample 5: .busstlang.to.be.onswirin\n",
      "Word 2 - Sample 6: .ourstheng.toobe.answer.d\n",
      "Word 2 - Sample 7: .ru.st.ond.tombe.answare.\n",
      "Word 2 - Sample 8: .qurstonnd.to.be.wnswaran\n",
      "Word 2 - Sample 9: .buasthend.tonbe.answorew\n",
      "\n",
      "Word 3 - Sample 0: .il.at..gashone.ge.rwing\n",
      "Word 3 - Sample 1: .it.ats.hathans.fear.ing\n",
      "Word 3 - Sample 2: .ilmath.cathind.hetrsing\n",
      "Word 3 - Sample 3: .ighaty.gawhend.begrling\n",
      "Word 3 - Sample 4: .ineath.pathane.mebrsing\n",
      "Word 3 - Sample 5: .ithath.watheno.berrhing\n",
      "Word 3 - Sample 6: .in.ath.sathont.te.rping\n",
      "Word 3 - Sample 7: .in.at..cashind.teprding\n",
      "Word 3 - Sample 8: .i.mate.wathens.searning\n",
      "Word 3 - Sample 9: .in.ate.tathin..searling\n",
      "\n",
      "Word 4 - Sample 0: .qurit.thaz.mby.it.is.hay.ar.\n",
      "Word 4 - Sample 1: .quant.thoz.tfoust.ad.wed.wa.\n",
      "Word 4 - Sample 2: .qurst.hezz.bupait.ly.ir..pl.\n",
      "Word 4 - Sample 3: .qu..t..caz.worert.f..ote.ca.\n",
      "Word 4 - Sample 4: .qugot.idiz.pule.t.wh.d.s.ng.\n",
      "Word 4 - Sample 5: .qught.esiz.tosast.to.tat.mo.\n",
      "Word 4 - Sample 6: .qulit.usiz.thtont.ts.trn.he.\n",
      "Word 4 - Sample 7: .qugat.whiz.s.chat.wa.ier.gn.\n",
      "Word 4 - Sample 8: .qus.t.ilez.in.a.t.lf.d.s.ar.\n",
      "Word 4 - Sample 9: .qulat..tiz.lealit.he.she.s..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in test_strings:\n",
    "    for i in range(10):\n",
    "        print \"Word \" + str(test_strings.index(word) + 1) + \" - Sample \" + str(i) + \": \" + f2(word)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f3(text):\n",
    "    text='.'+text\n",
    "    textnew=''\n",
    "    lastLetter=9999\n",
    "    totalProb=1\n",
    "    if text[len(text)-1]==\"_\":\n",
    "        for i in range(len(text)):\n",
    "            if text[i]!=\"_\":\n",
    "                lastLetter=i\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '_' :\n",
    "            matrix1=T\n",
    "            matrix2=T\n",
    "            matrix3=T\n",
    "            if i > lastLetter:\n",
    "                prob=matrix1[alphabet.index(textnew[i-1])]\n",
    "            else:\n",
    "                for j in range(len(text)):\n",
    "                    if j > i and text[j]!='_':\n",
    "                        ind2=j\n",
    "                        break\n",
    "                if ind2 - i > 1:\n",
    "                    for k in range(ind2 - i - 1):\n",
    "                        matrix2=np.dot(matrix2,T)\n",
    "                for k in range(ind2 - (i - 1) - 1):\n",
    "                    matrix3=np.dot(matrix3,T)\n",
    "                prob=np.multiply(matrix1[alphabet.index(textnew[i-1])],matrix2[:,alphabet.index(text[ind2])])/matrix3[alphabet.index(textnew[i-1]),alphabet.index(text[ind2])]\n",
    "            totalProb=totalProb*max(prob)\n",
    "            letter=alphabet[int(np.where(prob==max(prob))[0])]\n",
    "            textnew=textnew+letter\n",
    "        else:\n",
    "            textnew=textnew+text[i]\n",
    "    return textnew, totalProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: .the.br.an.fex.\n",
      "log-probability: -3.07433488138\n",
      "\n",
      "Word 2: .oursthend.to.be.answere.\n",
      "log-probability: -10.8154510862\n",
      "\n",
      "Word 3: .in.ath.wathend.he.r.ing\n",
      "log-probability: -11.6360900332\n",
      "\n",
      "Word 4: .qur.t.thiz.the.at.an.the.an.\n",
      "log-probability: -22.9236427638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in test_strings:\n",
    "    print \"Word \" + str(test_strings.index(word) + 1) + \": \" + str(f3(word)[0])\n",
    "    print \"log-probability:\", np.log(f3(word)[1])\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To improve the model for a better performance, we can use Markov(i) where i is greater than 1, such as Markov(4). This is because the probability of letters do not depend only on the previous letter, they can also depend on the ones before those or the ones coming after itself. So, a higher Markov model would give better performances, although the complexity would also be higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
