{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Burak Suyunu\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr?gr?mm?ng?H?m?w?rk 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "* Converted the string list into a float list and normalized to have a smoother distribution\n",
    "* Then used each distribution as a parameter to np.random.choice() function to choose a letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "Tf = []\n",
    "\n",
    "for t in T:\n",
    "    tf = [float(i) for i in t]\n",
    "    sm = sum(tf)\n",
    "    tff = [i/sm for i in tf]\n",
    "    Tf.append(tff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fun(N):\n",
    "    text = \".\"\n",
    "    for i in range(N):\n",
    "        chrc = np.random.choice(alphabet, 1, p=Tf[letter2idx[text[i]]])\n",
    "        text = text + chrc[0]\n",
    "    print(text[1:len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".the.thed.f.temenowandin.capod.ss.ors.whousedourede.and.o.momenththe.itinigrs.hen.amant.sun.of..whol\n"
     ]
    }
   ],
   "source": [
    "fun(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "To find the missing letters in this Markov(1) model we need to make an inference mechanism taht considers both the starting and the ending letters of the gap. Such inferenece can be represented as:\n",
    "\n",
    "$p(X_1 | X_0 = x_0, X_{N+1} = x_{N+1}) \\sim \\sum_{X_2:X_N} p(X_{N+1} = x_{N+1} | X_N) \\,\\, p(X_N | X_{N-1}) \\,\\, ... \\,\\, p(X_2 | X_1) \\,\\, p(X_1 | X_0 = x_0)$\n",
    "\n",
    "We can extract $p(X_1 | X_0 = x_0)$ part out of summation and to calculate rest of the summation, we need to take $N^{th}$ power of the transition matrix $(T^N)$. At the end, to find the probability distribution, we will multiply the row vector $\\big(p(X_1 | X_0 = x_0)\\big)$ obtained from $T$ with the column vector (The summation part) obtained from $(T^N)$. Then we will iteratively continue this process to find other missing letters from the gap.\n",
    "\n",
    "For Part 2 we will choose letters randomly from the obtained distributions. On the other hand, we will choose the most probable letter from the distribution for Part 3. This will be the only difference (also calculating log-probability) between Part 2 and Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precalculating the powers of T and store them\n",
    "\n",
    "Tpowers = []\n",
    "Tpowers.append(1)\n",
    "\n",
    "maxWordLength = len(test_strings[0])\n",
    "for testString in test_strings:\n",
    "    if len(testString) > maxWordLength:\n",
    "        maxWordLength = len(testString)\n",
    "        \n",
    "for i in range(1, maxWordLength):\n",
    "    p = np.dot(Tpowers[i-1], Tf)\n",
    "    Tpowers.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: th__br__n.f_x.\n",
      "Sample1 : thnubr.in.fex.\n",
      "Sample2 : the.broan.f.x.\n",
      "Sample3 : the.br.an.fex.\n",
      "Sample4 : thitbrain.fex.\n",
      "Sample5 : tho.bryon.fex.\n",
      "Sample6 : the.bre.n.fex.\n",
      "Sample7 : theabrean.fix.\n",
      "Sample8 : the.brean.fix.\n",
      "\n",
      "Original: _u_st__n_.to_be._nsw_r__\n",
      "Sample1 : ounsthani.to.be.inswhr.s\n",
      "Sample2 : ounsthany.to.be.wnswirwe\n",
      "Sample3 : susstsan..to.be..nswernd\n",
      "Sample4 : dumsthent.tombe.answoren\n",
      "Sample5 : butsteing.to.be.answorou\n",
      "Sample6 : oumstheny.to.be.answaris\n",
      "Sample7 : surstlend.to.be.ensworoo\n",
      "Sample8 : cursthind.to.be.onsw.rin\n",
      "\n",
      "Original: i__at_._a_h_n_._e_r_i_g\n",
      "Sample1 : ierato.lathand.hedrsing\n",
      "Sample2 : in.ato.wathind.berrning\n",
      "Sample3 : ircath.ha.hing.befrning\n",
      "Sample4 : ishate.dathind.beereing\n",
      "Sample5 : ishatw.fathine.leereing\n",
      "Sample6 : i.fath.wathant.hearding\n",
      "Sample7 : ineath.jathen..we.rhing\n",
      "Sample8 : ie.ate.mathant.hepreing\n",
      "\n",
      "Original: q___t.___z._____t.__.___.__.\n",
      "Sample1 : quint.whaz.han.ft.id.lle.th.\n",
      "Sample2 : qutat.byoz.to.but.ss.at..we.\n",
      "Sample3 : qud.t.an.z.teat.t.pe.whe.ow.\n",
      "Sample4 : qutat.spuz.ine.at.ts.ti..ho.\n",
      "Sample5 : que.t.ifaz.ievent.te.pes.by.\n",
      "Sample6 : quprt.dirz.be.n.t.me.ito.in.\n",
      "Sample7 : qu.at.aniz.ar.d.t.ms.ril.ss.\n",
      "Sample8 : qud.t.pliz..yilat.as..ge.is.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finds the probability distribution for the given situation\n",
    "def findProbDist(firstLetterInd, gap, lastLetterInd):\n",
    "    probDist = np.multiply( Tf[firstLetterInd] , Tpowers[gap][: , lastLetterInd] )\n",
    "    probDist = probDist / sum(probDist)\n",
    "    return probDist\n",
    "\n",
    "\n",
    "trials = 8\n",
    "\n",
    "for testString in test_strings:\n",
    "    print(\"Original: \" + testString)\n",
    "    \n",
    "    for j in reversed(range(len(testString))):\n",
    "        if (testString[j] != '_' and testString[j] != '?'):\n",
    "            lastLetterInd = j\n",
    "            break\n",
    "    \n",
    "    testString = '.' + testString\n",
    "    \n",
    "    for i in range(trials):\n",
    "        newWord = ''\n",
    "        \n",
    "        # When we enter a gap, initialize inthegap = gap distance\n",
    "        inthegap = 0\n",
    "        \n",
    "        for j in range(len(testString)):\n",
    "            if testString[j] != '_' and testString[j] != '?':\n",
    "                newWord = newWord + testString[j]\n",
    "                \n",
    "            elif j > lastLetterInd:\n",
    "                chrc = np.random.choice(alphabet, 1, p=Tf[letter2idx[newWord[j-1]]])\n",
    "                newWord = newWord + chrc[0]\n",
    "                \n",
    "            else:\n",
    "                # uses inthegap variable to avoid calculating gap distance in every iteration\n",
    "                if inthegap > 0:\n",
    "                    probDist =  findProbDist(letter2idx[newWord[j-1]], inthegap, letter2idx[gapLastLetter])\n",
    "                    \n",
    "                    chrc = np.random.choice(alphabet, 1, p=probDist)\n",
    "                    newWord = newWord + chrc[0]\n",
    "                    inthegap = inthegap - 1\n",
    "                    \n",
    "                else:\n",
    "                    inthegap = 0\n",
    "                    for k in range(j, len(testString)):\n",
    "                        if testString[k] == '_' or testString[k] == '?':\n",
    "                            inthegap = inthegap + 1\n",
    "                        else:\n",
    "                            gapLastLetter = testString[k]\n",
    "                            break\n",
    "                            \n",
    "                    probDist =  findProbDist(letter2idx[newWord[j-1]], inthegap, letter2idx[gapLastLetter])\n",
    "                    \n",
    "                    chrc = np.random.choice(alphabet, 1, p=probDist)\n",
    "                    newWord = newWord + chrc[0]\n",
    "                    inthegap = inthegap - 1\n",
    "                    \n",
    "        print(\"Sample\" + str(i+1) + \" : \" + newWord[1:len(newWord)])\n",
    "                    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "We are going to apply the same processes with Part 2 but now we will choose the most probable letters from the distributions rather than randomizing. We will be also calculating the log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original : th__br__n.f_x.\n",
      "Best Fit : the.br.an.fex.\n",
      "Log-Prob : -3.07433488138\n",
      "\n",
      "Original : _u_st__n_.to_be._nsw_r__\n",
      "Best Fit : oursthend.to.be.answerch\n",
      "Log-Prob : -8.32284546031\n",
      "\n",
      "Original : i__at_._a_h_n_._e_r_i_g\n",
      "Best Fit : in.ath.wathend.he.r.ing\n",
      "Log-Prob : -11.6360900332\n",
      "\n",
      "Original : q___t.___z._____t.__.___.__.\n",
      "Best Fit : qur.t.thiz.the.at.an.the.an.\n",
      "Log-Prob : -22.9236427638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for testString in test_strings:\n",
    "    print(\"Original : \" + testString)\n",
    "    \n",
    "    for j in reversed(range(len(testString))):\n",
    "        if (testString[j] != '_' and testString[j] != '?'):\n",
    "            lastLetterInd = j\n",
    "            break\n",
    "    \n",
    "    testString = '.' + testString\n",
    "        \n",
    "    newWord = ''\n",
    "    logProb = 1\n",
    "    \n",
    "    # When we enter a gap, initialize inthegap = gap distance\n",
    "    inthegap = 0\n",
    "    \n",
    "    for j in range(len(testString)):\n",
    "        if testString[j] != '_' and testString[j] != '?':\n",
    "            newWord = newWord + testString[j]\n",
    "            \n",
    "        elif j > lastLetterInd:\n",
    "            chrc = np.random.choice(alphabet, 1, p=Tf[letter2idx[newWord[j-1]]])\n",
    "            newWord = newWord + chrc[0]\n",
    "            \n",
    "        else:\n",
    "            # uses inthegap variable to avoid calculating gap distance in every iteration\n",
    "            if inthegap > 0:\n",
    "                probDist =  findProbDist(letter2idx[newWord[j-1]], inthegap, letter2idx[gapLastLetter])\n",
    "\n",
    "                logProb = logProb * max(probDist)\n",
    "                \n",
    "                newWord = newWord + alphabet[np.argmax(probDist)]\n",
    "                inthegap = inthegap - 1\n",
    "                \n",
    "            else:\n",
    "                inthegap = 0\n",
    "                for k in range(j, len(testString)):\n",
    "                    if testString[k] == '_' or testString[k] == '?':\n",
    "                        inthegap = inthegap + 1\n",
    "                    else:\n",
    "                        gapLastLetter = testString[k]\n",
    "                        break\n",
    "                        \n",
    "                probDist =  findProbDist(letter2idx[newWord[j-1]], inthegap, letter2idx[gapLastLetter])\n",
    "                logProb = logProb * max(probDist)\n",
    "\n",
    "                newWord = newWord + alphabet[np.argmax(probDist)]\n",
    "                inthegap = inthegap - 1\n",
    "                \n",
    "    print(\"Best Fit : \" + newWord[1:len(newWord)])\n",
    "    print(\"Log-Prob : \" + str(np.log(logProb)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "As an improvment we may use a higher order Markov rather than Markov(1). With using a higher order we can observe more complex structers in the words. So we can find better results. However using a higher order Markov's cost is adding up exponentially. So we should find a balance between precision and efficiency."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
