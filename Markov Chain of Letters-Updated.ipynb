{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Name: Yiğit Özgümüş\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Math, Latex\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#InteractiveShell.ast_node_interactivity = \"last_expr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Programming Homework 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['th__br__n.f_x.',\n",
       " '_u_st__n_.to_be._nsw_r__',\n",
       " 'i__at_._a_h_n_._e_r_i_g',\n",
       " 'q___t.___z._____t.__.___.__.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "test_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "idx2letter = {i:c for i,c in enumerate(alphabet)}\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reformat data for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9949749"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>...</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.036904</td>\n",
       "      <td>0.042629</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.007574</td>\n",
       "      <td>0.017138</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.037266</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110477</td>\n",
       "      <td>0.093429</td>\n",
       "      <td>0.131796</td>\n",
       "      <td>0.009803</td>\n",
       "      <td>0.030657</td>\n",
       "      <td>0.008880</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.023370</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.071522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.058003</td>\n",
       "      <td>0.005870</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.341671</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.047908</td>\n",
       "      <td>0.007689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074030</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.010743</td>\n",
       "      <td>0.119613</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.122984</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.021545</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.171592</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170172</td>\n",
       "      <td>0.056549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037681</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.090676</td>\n",
       "      <td>0.035836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.015177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.008109</td>\n",
       "      <td>0.122483</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.005484</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.079490</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017892</td>\n",
       "      <td>0.030704</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.017881</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.054587</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.022432</td>\n",
       "      <td>0.084343</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.012719</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130752</td>\n",
       "      <td>0.071279</td>\n",
       "      <td>0.024154</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.352561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         b         c         d         e         f         g  \\\n",
       "a  0.000284  0.022830  0.036904  0.042629  0.001222  0.007574  0.017138   \n",
       "b  0.058003  0.005870  0.000079  0.002263  0.341671  0.000206  0.000427   \n",
       "c  0.122984  0.000027  0.021545  0.000525  0.171592  0.000009  0.000000   \n",
       "d  0.028035  0.000506  0.000258  0.008109  0.122483  0.000680  0.005484   \n",
       "e  0.054587  0.001280  0.022432  0.084343  0.031710  0.008564  0.005283   \n",
       "\n",
       "          h         i         j    ...            r         s         t  \\\n",
       "a  0.001466  0.037266  0.000235    ...     0.110477  0.093429  0.131796   \n",
       "b  0.000364  0.047908  0.007689    ...     0.074030  0.022688  0.010743   \n",
       "c  0.170172  0.056549  0.000000    ...     0.037681  0.001049  0.090676   \n",
       "d  0.000708  0.079490  0.000348    ...     0.017892  0.030704  0.000916   \n",
       "e  0.001776  0.012719  0.000260    ...     0.130752  0.071279  0.024154   \n",
       "\n",
       "          u         v         w         x         y         z         .  \n",
       "a  0.009803  0.030657  0.008880  0.000956  0.023370  0.001870  0.071522  \n",
       "b  0.119613  0.001155  0.000032  0.000000  0.086450  0.000000  0.007452  \n",
       "c  0.035836  0.000000  0.000000  0.000000  0.004197  0.000009  0.015177  \n",
       "d  0.017881  0.002776  0.001365  0.000000  0.007648  0.000000  0.620154  \n",
       "e  0.001429  0.015731  0.007088  0.010514  0.012600  0.000183  0.352561  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_new = np.matrix(T).reshape(27,27);\n",
    "import string\n",
    "data1 =pd.DataFrame(data=T_new,index=list(alphabet),columns=list(alphabet));\n",
    "data =data1.apply(pd.to_numeric,errors='coerce')\n",
    "# P(x_t = 'u'| x_t-1 = 'q')\n",
    "data['u']['q']\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can rewrite the probability formula $p(x_{1:N}|x_0)$ as \n",
    "$$ p(x_{1:N}\\mid x_0=\\hat{x_0}) \\sim P(x_1\\mid x_0=\\hat{x_0}) P(x_2\\mid x_1) P(x_3\\mid x_2)...P(x_n\\mid x_n-1)$$\n",
    "\n",
    "Then we can choose each letter using the np.random.choice function iterating over the lenght of the desired string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Consider x0 as letter .\n",
    "def get_sample(N, x0 = '.'):\n",
    "    result = []\n",
    "    result.append(x0) # x0 choice\n",
    "    for i in range(N):\n",
    "        prob = data.loc[result[-1]]\n",
    "        prob_norm = prob / prob.sum(axis=0)\n",
    "        choice = np.random.choice(alphabet, p = list(prob_norm))\n",
    "        result.append(choice)\n",
    "    return ''.join(result[1:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..ts.r..hat.jalye.h.tllindreed.inend.ead.alo.ps.pe'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'thince..wemyouly.ad.ig.hefack.osan.ad.d.t.hat.joor'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'and.p.sthacee..tarinth.thend.steinymincas.mureddis'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'.abl.ve.y.tof.t.hun.chatonenousellnol.nto.gr.mpe.e'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'e.urgealan.butataf.fasme.pit.bjavaroth.leviningedo'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    get_sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's assume that we have string \"a__t\" which has 2 unknown values. Under the Markov(l) model assumption, for the second and third letter, we have the last known letter which is first and the first known letter after the unknowns which is the fourth to use in the distribution decision. For this problem, I assumed that the first and the last letter of every given sentence is known so I added colons to each side of the sentences. Then if we have a given string length of N, after the addition of colons, the problem transforms into something like below:\n",
    "\n",
    "$$p(x_{1:N}\\mid x_0 = \\hat{x_0},x_{n+1}=\\hat{x_{n+1}}) \\sim p(x_{n+1}=\\hat{x_{n+1}}\\mid x_{n}) p(x_{n}\\mid x_{n-1})p(x_{n-1}\\mid x_{n-2})p(x_{n-2}\\mid x_{n-3})...p(x_{2}\\mid x_{1})p(x_{1}\\mid x_{0}=\\hat{x_0})$$\n",
    "\n",
    "We can obtain the last probability in the iteration directly from the transition matrix, for the rest of the bundle, we can take the $N'th$ power of the transition matrix and get the distribution vector regarding the $x_{n+1}$. Elementwise multiplication of these two vectors will give us the probability vector we want. Then we can normalize this vector and we can use np.random.choice method again to predict the missing letter. This strategy will be used iteratively from the start of the string and each predicted letter will be contributed to the calculation in the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String 1 :  th__br__n.f_x.\n",
      "Sample  1 = the.brean.fex.\n",
      "Sample  2 = tht.brion.fax.\n",
      "Sample  3 = the.br..n.fix.\n",
      "Sample  4 = thembrton.fex.\n",
      "Sample  5 = the.br.an.fex.\n",
      "Sample  6 = the.brsan.fex.\n",
      "Sample  7 = the.br.on.fex.\n",
      "Sample  8 = thambryon.fex.\n",
      "Sample  9 = the.brean.fex.\n",
      "Sample  10 = the.brd.n.fix.\n",
      "Test String 2 :  _u_st__n_.to_be._nsw_r__\n",
      "Sample  1 = ouesthend.tombe.inswar.s\n",
      "Sample  2 = mubsthane.tombe.answer.t\n",
      "Sample  3 = ouisthand.to.be.answer.f\n",
      "Sample  4 = musst.ang.tombe.answaris\n",
      "Sample  5 = ou.stignn.to.be.onswer.s\n",
      "Sample  6 = sunsthiny.to.be.enswered\n",
      "Sample  7 = cussthend.to.be.answerin\n",
      "Sample  8 = ouist.ing.to.be.onswer.h\n",
      "Sample  9 = mussttinf.toube.inswar.d\n",
      "Sample  10 = bulsthond.tombe..nswor.s\n",
      "Test String 3 :  i__at_._a_h_n_._e_r_i_g\n",
      "Sample  1 = is.ate.washene.meerwing\n",
      "Sample  2 = io.ath.bathong.rearaing\n",
      "Sample  3 = is.ats.sa.hand.te.rdi.g\n",
      "Sample  4 = ighats.ma.hend.petrding\n",
      "Sample  5 = ielaty.rathind.setr.ing\n",
      "Sample  6 = in.atl.tatheng.jear.ing\n",
      "Sample  7 = icuatt.hathend.setr.ing\n",
      "Sample  8 = ie.ata.wathend.tedr.ing\n",
      "Sample  9 = in.ath.jathang.me.r.ing\n",
      "Sample  10 = ichato.wa.hand.beor.i.g\n",
      "Test String 4 :  q___t.___z._____t.__.___.__.\n",
      "Sample  1 = que.t.warz.t.to.t.mo.ror.at.\n",
      "Sample  2 = qul.t.omiz...s.ot.ad.ble.ll.\n",
      "Sample  3 = qugat.tooz.asthet.or.d.d.an.\n",
      "Sample  4 = qut.t.herz.hal.ct.te.she.hi.\n",
      "Sample  5 = quspt.anoz.inge.t.as.g.t.hs.\n",
      "Sample  6 = quaut.tinz.thefit.an.ous.be.\n",
      "Sample  7 = qup.t.turz.epes.t.an.men.by.\n",
      "Sample  8 = qustt.herz.thequt.ar.amy.ta.\n",
      "Sample  9 = qupat.heaz.sh.apt.on.and.ar.\n",
      "Sample  10 = qurct.derz.is.ant.ar.the.ag.\n"
     ]
    }
   ],
   "source": [
    "def make_dataframe(matrix):\n",
    "    new_df =pd.DataFrame(data=matrix,index=list(alphabet),columns=list(alphabet));\n",
    "    new_df =new_df.apply(pd.to_numeric,errors='coerce')\n",
    "    return new_df\n",
    "\n",
    "def normalize(vector):\n",
    "    return vector / vector.sum(axis=0)\n",
    "\n",
    "def guess_the_blank(index,sentence,probability):\n",
    "    arrayified = list(sentence)\n",
    "    new_guess = np.random.choice(alphabet,p=probability)\n",
    "    arrayified[index] = new_guess\n",
    "    return ''.join(arrayified)\n",
    "\n",
    "def fill_the_blanks(sentence):\n",
    "    sentence = \".\" + sentence + \".\"\n",
    "    #print(sentence)\n",
    "    while (\"_\" or \"?\") in sentence:\n",
    "        index = sentence.index(\"_\" or \"?\")\n",
    "        prior_index = index-1\n",
    "        pr_letter = sentence[prior_index]\n",
    "        #print(\"pri: \",pr_letter)\n",
    "        post_index = 0\n",
    "        for k in range(index,len(sentence)):\n",
    "         #   print(\"k\",k)\n",
    "          #  print(\"letter is: \",sentence[k])\n",
    "            if(sentence[k] != \"_\"):\n",
    "                post_index = k \n",
    "                break\n",
    "        #print(\"------\")\n",
    "        po_letter = sentence[post_index]\n",
    "       # print(\"po: \",po_letter)\n",
    "        length = post_index - prior_index-1\n",
    "        powered = np.linalg.matrix_power(data.as_matrix(),length)\n",
    "        powered_df =make_dataframe(powered)\n",
    "        probability = normalize(powered_df[po_letter] * data.loc[pr_letter])\n",
    "        sentence = guess_the_blank(index,sentence,probability)\n",
    "        #print(\"guessed sentence: \",sentence[1:-1])\n",
    "    return sentence[1:-1]\n",
    "\n",
    "for i in range(len(test_strings)):\n",
    "    print(\"Test String \" + str(i+1),\": \", test_strings[i])\n",
    "    for j in range(10):\n",
    "        print(\"Sample \",str(j+1), \"=\", fill_the_blanks(test_strings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The same solution strategy at the task 2 is used here. The only difference is instead of choosing with np.random.choice, we choose the most likely one, in this case the letter with the highest probability. The summation of the log probabilities are also shown for each test string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test String 1 :  th__br__n.f_x.\n",
      "Prediction  = the.br.an.fex.\n",
      "Log probability:  -3.07433486577 \n",
      "\n",
      "Test String 2 :  _u_st__n_.to_be._nsw_r__\n",
      "Prediction  = oursthend.to.be.answered\n",
      "Log probability:  -11.0693279723 \n",
      "\n",
      "Test String 3 :  i__at_._a_h_n_._e_r_i_g\n",
      "Prediction  = in.ath.wathend.he.r.ing\n",
      "Log probability:  -11.636089996 \n",
      "\n",
      "Test String 4 :  q___t.___z._____t.__.___.__.\n",
      "Prediction  = qur.t.thiz.the.at.an.the.an.\n",
      "Log probability:  -22.9236424228 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_dataframe(matrix):\n",
    "    new_df =pd.DataFrame(data=matrix,index=list(alphabet),columns=list(alphabet));\n",
    "    new_df =new_df.apply(pd.to_numeric,errors='coerce')\n",
    "    return new_df\n",
    "\n",
    "def normalize(vector):\n",
    "    return vector / vector.sum(axis=0)\n",
    "\n",
    "def guess_the_blank_mle(index,sentence,probability):\n",
    "    arrayified = list(sentence)\n",
    "    new_guess = np.argmax(probability)\n",
    "    arrayified[index] = new_guess\n",
    "    log_prob = np.log(probability[new_guess])\n",
    "    return ''.join(arrayified),log_prob\n",
    "\n",
    "def fill_the_blanks3(sentence):\n",
    "    sentence = \".\" + sentence + \".\"\n",
    "    log_probability = 0\n",
    "    #print(sentence)\n",
    "    while (\"_\" or \"?\") in sentence:\n",
    "        index = sentence.index(\"_\" or \"?\")\n",
    "        prior_index = index-1\n",
    "        pr_letter = sentence[prior_index]\n",
    "        #print(\"pri: \",pr_letter)\n",
    "        post_index = 0\n",
    "        for k in range(index,len(sentence)):\n",
    "         #   print(\"k\",k)\n",
    "          #  print(\"letter is: \",sentence[k])\n",
    "            if(sentence[k] != \"_\"):\n",
    "                post_index = k \n",
    "                break\n",
    "        #print(\"------\")\n",
    "        po_letter = sentence[post_index]\n",
    "       # print(\"po: \",po_letter)\n",
    "        length = post_index - prior_index-1\n",
    "        powered = np.linalg.matrix_power(data.as_matrix(),length)\n",
    "        powered_df =make_dataframe(powered)\n",
    "        probability = normalize(powered_df[po_letter] * data.loc[pr_letter])\n",
    "        sentence,temp_log_prob = guess_the_blank_mle(index,sentence,probability)\n",
    "        log_probability += temp_log_prob\n",
    "        #print(\"guessed sentence: \",sentence[1:-1])\n",
    "    return sentence[1:-1], log_probability\n",
    "\n",
    "for i in range(len(test_strings)):\n",
    "    print(\"Test String \" + str(i+1),\": \", test_strings[i])\n",
    "    prediction, logP = fill_the_blanks3(test_strings[i])\n",
    "    print(\"Prediction \", \"=\", prediction)\n",
    "    print(\"Log probability: \",logP,\"\\n\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Considering the language, higher Markov model could give us better estimation in terms of similarity to english language. The 1 letter distance in the current model occasionaly can create meaningful words such as \"the\" with randomization approach. It gave relatively better results with most likely letter esti, but with the increased level of distance in the model, we can estimate better values and obtain a better similarity. But increasing the Markov model will also increase the complexity of the computation model itself. So there will be a certain trade off to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
