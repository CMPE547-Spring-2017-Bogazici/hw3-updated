{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Name: Kemal Berk Kocabagli\n",
    "\n",
    "I hereby declare that I observed the honour code of the university when preparing the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Programming Homework 3\n",
    "\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider letters 'a-z'. Capital letters 'A-Z' are mapped to the corresponding ones. All remaining letters, symbols, numbers, including spaces, are denoted by '.'.\n",
    "\n",
    "\n",
    "We have a probability table $T$ where $T_{i,j} = p(x_t = j | x_{t-1} = i)$  transition model of letters in English text for $t=1,2 \\dots N$. Assume that the initial letter in a string is always a space denoted as $x_0 = \\text{'.'}$. Such a model where the probability table is always the same is sometimes called a stationary model.\n",
    "\n",
    "1. For a given $N$, write a program to sample random strings with letters $x_1, x_2, \\dots, x_N$ from $p(x_{1:N}|x_0)$\n",
    "1. Now suppose you are given strings with missing letters, where each missing letter is denoted by a question mark (or underscore, as below). Implement a method, that samples missing letters conditioned on observed ones, i.e., samples from $p(x_{-\\alpha}|x_{\\alpha})$ where $\\alpha$ denotes indices of observed letters. For example, if the input is 't??.', we have $N=4$ and\n",
    "$x_1 = \\text{'t'}$ and $x_4 = \\text{'.'}$, $\\alpha=\\{1,4\\}$ and $-\\alpha=\\{2,3\\}$. Your program may possibly generate the strings 'the.', 'twi.', 'tee.', etc. Hint: make sure to make use all data given and sample from the correct distribution. Implement the method and print the results for the test strings below. \n",
    "1. Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "1. Discuss how you can improve the model to get better estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0002835\n",
      "b 0.0228302\n",
      "c 0.0369041\n",
      "d 0.0426290\n",
      "e 0.0012216\n",
      "f 0.0075739\n",
      "g 0.0171385\n",
      "h 0.0014659\n",
      "i 0.0372661\n",
      "j 0.0002353\n",
      "k 0.0110124\n",
      "l 0.0778259\n",
      "m 0.0260757\n",
      "n 0.2145354\n",
      "o 0.0005459\n",
      "p 0.0195213\n",
      "q 0.0001749\n",
      "r 0.1104770\n",
      "s 0.0934290\n",
      "t 0.1317960\n",
      "u 0.0098029\n",
      "v 0.0306574\n",
      "w 0.0088799\n",
      "x 0.0009562\n",
      "y 0.0233701\n",
      "z 0.0018701\n",
      ". 0.0715219\n"
     ]
    }
   ],
   "source": [
    "import csv # 27x27 table, stationary since x0 is fixed to be '.'\n",
    "import numpy as np\n",
    "from itertools import product # for cartesian product\n",
    "import operator # for max index\n",
    "\n",
    "\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row) # add probabilities to T row by row\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are asked to sample the letter from\n",
    "\n",
    "\\begin{equation}\n",
    "P = p(x_1,x_2...x_n|x_0='.') = \\dfrac{p(x_0='.',x_1,x_2\\dots,x_n)}{p(x_0='.')} = p(x_1|x_0='.') \\cdotp p(x_2|x_1) \\dots \\cdotp p(x_n|x_{n-1})\n",
    "\\end{equation}\n",
    "\n",
    "Since our model is Markov(1),\n",
    "only the immediately preceding letter affects the probability distribution of the current letter.\n",
    "\n",
    "P for letter $x_i$ is\n",
    "\\begin{equation}\n",
    "P_{x_i} = p(x_i|x_{i-1}=\\hat{x_{i-1}}) \n",
    "\\end{equation}\n",
    "\n",
    "Our strategy will be to sample one by one and take the corresponding row of the transition matrix every time for the next letter. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1\n",
      "Sample of length  1 :  s\n",
      "Sample of length  2 :  co\n",
      "Sample of length  3 :  ave\n",
      "Sample of length  4 :  trds\n",
      "Sample of length  5 :  d.wat\n",
      "Sample of length  6 :  sid.sc\n",
      "Sample of length  7 :  .aust.a\n",
      "Sample of length  8 :  therrise\n",
      "Sample of length  9 :  wlounvarr\n",
      "Sample of length  10 :  toi.imbath\n",
      "Sample of length  11 :  licowoach.h\n",
      "Sample of length  12 :  any.ofronell\n",
      "Sample of length  13 :  .f.nt.shtoto.\n",
      "Sample of length  14 :  thesto.vararok\n",
      "Sample of length  15 :  titondishethe.w\n",
      "Sample of length  16 :  irs.hevithe.icej\n",
      "Sample of length  17 :  pen.are.orethe.or\n",
      "Sample of length  18 :  o.as.iathicren.the\n",
      "Sample of length  19 :  .l.efaravere.lf.the\n",
      "Sample of length  20 :  tlarareto.vounowouer\n",
      "Sample of length  21 :  he.be.f.und.d.ben.cab\n",
      "Sample of length  22 :  th.st.fuloch.d..fobeth\n",
      "Sample of length  23 :  alo.e.heces.hious.t.iov\n",
      "Sample of length  24 :  wid.d.incenor.s.ou.alath\n",
      "Sample of length  25 :  weal.f.f.soundeanss.d.d.r\n",
      "Sample of length  26 :  wo.cheref.d.foul.mmead.tym\n",
      "Sample of length  27 :  th.se.m.d.sthe.ond.o.tlans.\n",
      "Sample of length  28 :  t.nis.dy.winnobjugowigroctth\n",
      "Sample of length  29 :  brbjushe.thinthe.touse.sum.be\n",
      "Sample of length  30 :  thryosslluthicha.mee.w.foy.wai\n"
     ]
    }
   ],
   "source": [
    "import csv # 27x27 table, stationary since x0 is fixed to be '.'\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = [] # transition matrix\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        row = [float(e) for e in row] # convert to float\n",
    "        T.append(row) # add probabilities to T row by row\n",
    "\n",
    "\n",
    "# algorithm for weighted choice\n",
    "def weighted_choice(weights):\n",
    "    totals = []\n",
    "    running_total = 0\n",
    "\n",
    "    for w in weights:\n",
    "        running_total += w\n",
    "        totals.append(running_total)\n",
    "\n",
    "    rnd = np.random.random() * running_total\n",
    "    for i, total in enumerate(totals):\n",
    "        if rnd < total:\n",
    "            return i\n",
    "        \n",
    "def sample(N):\n",
    "    # N = word length\n",
    "    x0 = '.' # x0 is space\n",
    "    randomWord= x0 \n",
    "    for i in range(N):\n",
    "        #print(\"current letter: \", randomWord[i])\n",
    "        p = T[letter2idx[randomWord[i]]] # get the probability of next letter given the current letter\n",
    "        p /= np.sum(p) # normalize just in case\n",
    "        randomWord+=(alphabet[weighted_choice(p)]) # get a letter with weighted probability and append it to the word \n",
    "        \n",
    "    return(randomWord[1:]) # return the word omitting the initial space character\n",
    "   \n",
    "# main\n",
    "print(\"Part 1\")\n",
    "for i in range(30):\n",
    "    print(\"Sample of length \",i+1 ,\": \",sample(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For this part, we can consider 'chunks' of missing letters.\n",
    "Define a chunk $C$ as $l_i x_1 x_2 x_3 \\dots x_n l_f$ where $l_i$ is the initial letter and $l_f$ is the final letter of the chunk, which we know (they are given). Only these two letters will affect the missing letters in between because of the Markov(1) property of d-separation.\n",
    "\n",
    "We will again go one by one in sampling the letters in a chunk. This time, however, once we sample $x_i$, we will have to recalculate taking $l_i =x_i$ and go until the end.\n",
    "\n",
    "To begin with, we will sample $x_1$ from (the factorization comes from Markov(1) factor graph)\n",
    "\n",
    "\\begin{equation}\n",
    "p(x_{1:n}|x_0=l_i,x_{n+1}=l_f) \\propto \\Big( p(x_1|x_0=l_i)* \\sum\\limits_{x_2:x_n}{p(x_2|x_1)* p(x_3|x_2) * \\dots * p(x_{n+1}=l_f|x_n)} \\Big) = p(x_1|x_0=l_i)* T^{n-1} \\cdotp p(x_{n+1}=l_f|x_n)\n",
    "\\end{equation}\n",
    "where $T$ is the transition matrix.\n",
    "\n",
    "At this point, we have chosen $x_1$ to be $\\hat{x_1}$.\n",
    "\n",
    "Then $x_2$ from \n",
    "\\begin{equation}\n",
    "p(x_{2:n}|x_1=\\hat{x_1},x_{n+1}=l_f) \\propto \\Big( p(x_2|x_1=\\hat{x_1})* \\sum\\limits_{x_3:x_n} p(x_3|x_2)* p(x_4|x_3) * \\dots * p(x_{n+1}=l_f|x_n) \\Big) = p(x_2|x_1=\\hat{x_1})* T^{n-2} \\cdotp p(x_{n+1}=l_f|x_n)\n",
    "\\end{equation}\n",
    "\n",
    "$\\dots$\n",
    "\n",
    "Finally, \n",
    "\\begin{equation}\n",
    "p(x_n|x_{n-1}=\\hat{x_{n-1}},x_{n+1}=l_f) \\propto p(x_n|x_{n-1}=\\hat{x_{n-1}})* p(x_{n+1}=l_f|x_n)\n",
    "\\end{equation}\n",
    "\n",
    "Once we are done with chunk 1, we will go to the next one until there is no chunk left to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2\n",
      "Original string:  th__br__n.f_x.\n",
      "Sample  1 : the.brern.fex.\n",
      "Sample  2 : the.brzan.fix.\n",
      "Sample  3 : thy.brsen.fex.\n",
      "Sample  4 : the.br.on.fox.\n",
      "Sample  5 : thoobrtin.fix.\n",
      "Sample  6 : the.brorn.fex.\n",
      "Sample  7 : the.br.on.fex.\n",
      "Sample  8 : the.brt.n.fex.\n",
      "Sample  9 : the.brain.fox.\n",
      "Sample  10 : the.br.an.fex.\n",
      "\n",
      "\n",
      "Original string:  _u_st__n_.to_be._nsw_r__\n",
      "Sample  1 : rursthind.torbe.rnswor.o\n",
      "Sample  2 : qussth.nd.tombe.wnswarle\n",
      "Sample  3 : pu.sthant.to.be.answer.r\n",
      "Sample  4 : fuist.ind.toube.answarof\n",
      "Sample  5 : outstheno.torbe.answ.r.t\n",
      "Sample  6 : bursthind.to.be.onswerte\n",
      "Sample  7 : cutstuind.to.be.onswarse\n",
      "Sample  8 : suastland.toube.answerat\n",
      "Sample  9 : ougstwano.to.be.wnswerod\n",
      "Sample  10 : bunsthen..toube.onswer.p\n",
      "\n",
      "\n",
      "Original string:  i__at_._a_h_n_._e_r_i_g\n",
      "Sample  1 : ineaty.waghind.beer.ing\n",
      "Sample  2 : ifrate.sathand.reirling\n",
      "Sample  3 : imaati.sathind.befrsing\n",
      "Sample  4 : is.ath.tatheno.rearying\n",
      "Sample  5 : il.ate.bachin..merr.ing\n",
      "Sample  6 : id.atl.wa.hwnd.tefreing\n",
      "Sample  7 : it.ath.pathent.ie.rting\n",
      "Sample  8 : is.atr.pathend.hear.ing\n",
      "Sample  9 : it.ath.fathene.bedr.ing\n",
      "Sample  10 : ishath.sathind.pearaing\n",
      "\n",
      "\n",
      "Original string:  q___t.___z._____t.__.___.__.\n",
      "Sample  1 : qu.at.maiz.herest.th.can.te.\n",
      "Sample  2 : qug.t.ourz.hensut.th.hin.tt.\n",
      "Sample  3 : qur.t.antz.m.as.t.pa.pof.ty.\n",
      "Sample  4 : qulit.herz.foisht.wh..as.ly.\n",
      "Sample  5 : qu.at.chaz.n.oust.se.f.b.at.\n",
      "Sample  6 : qunct.verz.t.tout.ch.len.te.\n",
      "Sample  7 : qug.t.serz.wisi.t.pr.nge.he.\n",
      "Sample  8 : qutht.ourz.toyw.t.he..ly.ff.\n",
      "Sample  9 : que.t..orz.pens.t.he.bes.ha.\n",
      "Sample  10 : qun.t.teiz.d.t.tt.ff.tum.ty.\n",
      "\n",
      "\n",
      "Here's something extra. This is what my algorithm thinks the homework title should be:\n",
      "Sample  1 : pragrimmong.him.werk.3\n",
      "Sample  2 : pragremmong.him.wark.3\n",
      "Sample  3 : pr.gr.mmang.him.wark.3\n",
      "Sample  4 : pr.grummang.h.mowark.3\n",
      "Sample  5 : programmeng.himewerk.3\n",
      "Sample  6 : pr.gremming.him.whrk.3\n",
      "Sample  7 : progromminggh.m.werk.3\n",
      "Sample  8 : prngrimmang.himowerk.3\n",
      "Sample  9 : prigr.mmanggh.mowerk.3\n",
      "Sample  10 : prngrimming.himowirk.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv # 27x27 table, stationary since x0 is fixed to be '.'\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = [] # transition matrix\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        row = [float(e) for e in row] # convert to float\n",
    "        T.append(row) # add probabilities to T row by row\n",
    "\n",
    "T = np.asarray(T) # convert T into array from list\n",
    "\n",
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "        \n",
    "def complete_word(incomplete_word):\n",
    "    N = len(incomplete_word)\n",
    "    a = []\n",
    "    a_not = []\n",
    "    for i in range(N):\n",
    "        if incomplete_word[i]=='_' or incomplete_word[i]=='?':\n",
    "            a_not.append(i)\n",
    "        else: \n",
    "            a.append(i)    \n",
    "    \n",
    "    if len(a_not)==0:\n",
    "        return incomplete_word\n",
    "    \n",
    "    missingchunks = [] \n",
    "    prev = a_not[0]-1\n",
    "    missingchunk=[] # indicates the indices of a missing chunk; (!) missingchunk indices do not include l_i and l_f\n",
    "    for i in a_not:\n",
    "        if (i==prev+1):\n",
    "            missingchunk.append(i)\n",
    "        else: \n",
    "            missingchunks.append(missingchunk)\n",
    "            missingchunk=[i]\n",
    "        if(i==a_not[len(a_not)-1]):\n",
    "                 missingchunks.append(missingchunk)\n",
    "        prev=i\n",
    "        \n",
    "    #print(missingchunks)\n",
    "    \n",
    "    # Think of this as factorization. A chunk consists of li_______lf, \n",
    "    # where li is the given initial letter and lf is the given final letter of the chunk.\n",
    "    # The letters in the next missing chunk do not depend on anything before its own li.\n",
    "    \n",
    "    # Markov(1) -> Given the present observation xi, the future is independent from the past(x{i-1},x{i-2}...)\n",
    "    \n",
    "    for mc in missingchunks: # for each missing chunk, fill in the blanks\n",
    "        #print(\"In chunk: \", mc)\n",
    "        li_index= mc[0]-1       \n",
    "        lf_index= mc[len(mc)-1]+1\n",
    "        \n",
    "        # bounds check. If no li pr lf, take them as '.'\n",
    "        if li_index<0:\n",
    "            li='.'\n",
    "        else:\n",
    "            li=incomplete_word[li_index]\n",
    "            \n",
    "        if lf_index>len(incomplete_word)-1:\n",
    "            lf='.'\n",
    "        else:\n",
    "            lf=incomplete_word[lf_index]\n",
    "            \n",
    "        #print(li,lf)\n",
    "        for i in range(len(mc)): # fill xi            \n",
    "            p = T[letter2idx[li]] # p(x1|li)\n",
    "            tpower=1\n",
    "            # Marginalization\n",
    "            for j in range (i,len(mc)-1):\n",
    "                tpower=np.dot(tpower,(T)) # p(x{i}|x{i-1});\n",
    "               \n",
    "            # at this point, p has info coming from li and the following dot product has info coming from lf.\n",
    "            # we combine these to yield final prob dist\n",
    "            p = p * np.dot(tpower,T[:,letter2idx[lf]]) # final prob dist = p(x1|li) * p(x1|lf)    \n",
    "            p /= sum(p) # normalize\n",
    "            \n",
    "            xi = alphabet[weighted_choice(p)] # pick xi from final prob dist\n",
    "\n",
    "            incomplete_word= incomplete_word[0:li_index+1+i]+xi+incomplete_word[li_index+i+2:]\n",
    "            li=xi # xi is the new li\n",
    "    \n",
    "    return(incomplete_word)\n",
    "        \n",
    "        \n",
    "\n",
    "# main\n",
    "print(\"Part 2\")\n",
    "sample_size=10\n",
    "for str in test_strings:\n",
    "    print(\"Original string: \", str)\n",
    "    for i in range(sample_size):\n",
    "        print(\"Sample \", i+1, \":\", complete_word(str))\n",
    "    print(\"\\n\")\n",
    "\n",
    "str = \"Pr?gr?mm?ng?H?m?w?rk.3\"\n",
    "print(\"Here's something extra. This is what my algorithm thinks the homework title should be:\")\n",
    "for i in range(sample_size):\n",
    "        print(\"Sample \", i+1, \":\", complete_word(str.lower()))\n",
    "print(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Everything is the same as in Part 2. The only difference is that after calculating the desired probability distribution for $x_i$, we will take the letter with the maximum probability. The cumulative sum of the log of the corresponding probabilities will be printed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 3\n",
      "Original string:  th__br__n.f_x.\n",
      "(log-probability, guess)= (-3.0743348657731824, 'the.br.an.fex.') \n",
      "\n",
      "Original string:  _u_st__n_.to_be._nsw_r__\n",
      "(log-probability, guess)= (-11.069327972319437, 'oursthend.to.be.answered') \n",
      "\n",
      "Original string:  i__at_._a_h_n_._e_r_i_g\n",
      "(log-probability, guess)= (-11.636089996019512, 'in.ath.wathend.he.r.ing') \n",
      "\n",
      "Original string:  q___t.___z._____t.__.___.__.\n",
      "(log-probability, guess)= (-22.923642422825658, 'qur.t.thiz.the.at.an.the.an.') \n",
      "\n",
      "Here's something extra. This is what my algorithm thinks the homework title should be:\n",
      "(log-probability, guess)= (-6.626367363404877, 'pr.gr.mmang.hem.werk.3') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv # 27x27 table, stationary since x0 is fixed to be '.'\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = [] # transition matrix\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        row = [float(e) for e in row] # convert to float\n",
    "        T.append(row) # add probabilities to T row by row\n",
    "\n",
    "T = np.asarray(T)  # convert T into array from list\n",
    "\n",
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']\n",
    "        \n",
    "def complete_word(incomplete_word):\n",
    "    logprob=0\n",
    "    N = len(incomplete_word)\n",
    "    a = []\n",
    "    a_not = []\n",
    "    for i in range(N):\n",
    "        if incomplete_word[i]=='_' or incomplete_word[i]=='?':\n",
    "            a_not.append(i)\n",
    "        else: \n",
    "            a.append(i)    \n",
    "    \n",
    "    if len(a_not)==0:\n",
    "        return (logprob,incomplete_word)\n",
    "    \n",
    "    missingchunks = [] \n",
    "    prev = a_not[0]-1\n",
    "    missingchunk=[] # indicates the indices of a missing chunk; (!) missingchunk indices do not include l_i and l_f\n",
    "    for i in a_not:\n",
    "        if (i==prev+1):\n",
    "            missingchunk.append(i)\n",
    "        else: \n",
    "            missingchunks.append(missingchunk)\n",
    "            missingchunk=[i]\n",
    "        if(i==a_not[len(a_not)-1]):\n",
    "                 missingchunks.append(missingchunk)\n",
    "        prev=i\n",
    "        \n",
    "    #print(missingchunks)\n",
    "    \n",
    "    # Think of this as factorization. A chunk consists of li_______lf, \n",
    "    # where li is the given initial letter and lf is the given final letter of the chunk.\n",
    "    # The letters in the next missing chunk do not depend on anything before its own li.\n",
    "    \n",
    "    # Markov(1) -> Given the present observation xi, the future is independent from the past(x{i-1},x{i-2}...)\n",
    "    \n",
    "    for mc in missingchunks: # for each missing chunk, fill in the blanks\n",
    "        #print(\"In chunk: \", mc)\n",
    "        li_index= mc[0]-1       \n",
    "        lf_index= mc[len(mc)-1]+1\n",
    "        \n",
    "        # bounds check. If no li pr lf, take them as '.'\n",
    "        if li_index<0:\n",
    "            li='.'\n",
    "        else:\n",
    "            li=incomplete_word[li_index]\n",
    "            \n",
    "        if lf_index>len(incomplete_word)-1:\n",
    "            lf='.'\n",
    "        else:\n",
    "            lf=incomplete_word[lf_index]\n",
    "            \n",
    "        #print(li,lf)\n",
    "        for i in range(len(mc)): # fill xi            \n",
    "            p = T[letter2idx[li]] # p(x1|li)\n",
    "            tpower=1\n",
    "            # Marginalization\n",
    "            for j in range (i,len(mc)-1):\n",
    "                tpower=np.dot(tpower,(T)) # p(x{i}|x{i-1});\n",
    "               \n",
    "            # at this point, p has info coming from li and the following dot product has info coming from lf.\n",
    "            # we combine these to yield final prob dist\n",
    "            p = p * np.dot(tpower,T[:,letter2idx[lf]]) # final prob dist = p(x1|li) * p(x1|lf)    \n",
    "            p /= sum(p) # normalize\n",
    "            \n",
    "            xi = alphabet[np.argmax(p)] # pick xi to be the letter with maximum probability\n",
    "            logprob+=np.log(p[np.argmax(p)]) # add log of this prob to the log cumulative sum\n",
    "\n",
    "            incomplete_word= incomplete_word[0:li_index+1+i]+xi+incomplete_word[li_index+i+2:]\n",
    "            li=xi # xi is the new li\n",
    "    \n",
    "    return(logprob,incomplete_word)\n",
    "        \n",
    "# main\n",
    "print(\"Part 3\")\n",
    "sample_size=10\n",
    "for str in test_strings:\n",
    "    print(\"Original string: \", str)\n",
    "    print(\"(log-probability, guess)=\",complete_word(str),\"\\n\")\n",
    "\n",
    "str = \"Pr?gr?mm?ng?H?m?w?rk.3\"\n",
    "print(\"Here's something extra. This is what my algorithm thinks the homework title should be:\")\n",
    "print(\"(log-probability, guess)=\",complete_word(str.lower()), \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4\n",
    "The question could be solved using the sum-product algorithm, which works for much more general cases than Markov(1) type graphs. In fact, using a deeper level Markov model would give better results in predicting the missing letters. A given letter might influence two, three, or even all of the future letters rather than just one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
