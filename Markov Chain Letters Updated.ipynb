{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Deniz Ekiz\n",
    "ID: 2016700051\n",
    "## Homework 3 Programming Part\n",
    "\n",
    "### Question\n",
    "In this exercise we model a string of text using a Markov(1) model. For simplicity we only consider\n",
    "letters ’a-z’. Capital letters ’A-Z’ are mapped to the corresponding ones. All remaining letters,\n",
    "symbols, numbers, including spaces, are denoted by ’.’.\n",
    "We have a probability table T where Ti,j = p(xt = j|xt−1 = i) transition model of letters in\n",
    "English text for t = 1, 2 . . . N. Assume that the initial letter in a string is always a space denoted\n",
    "as x0 = ’.’. Such a model where the probability table is always the same is sometimes called a\n",
    "stationary model.\n",
    "\n",
    "\n",
    "4. Discuss how you can improve the model to get better estimations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_strings = ['th__br__n.f_x.', '_u_st__n_.to_be._nsw_r__','i__at_._a_h_n_._e_r_i_g','q___t.___z._____t.__.___.__.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The code below loads a table of transition probabilities for English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9949749\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$p(x_t | x_{t-1} = \\text{'a'})$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '0.0002835')\n",
      "('b', '0.0228302')\n",
      "('c', '0.0369041')\n",
      "('d', '0.0426290')\n",
      "('e', '0.0012216')\n",
      "('f', '0.0075739')\n",
      "('g', '0.0171385')\n",
      "('h', '0.0014659')\n",
      "('i', '0.0372661')\n",
      "('j', '0.0002353')\n",
      "('k', '0.0110124')\n",
      "('l', '0.0778259')\n",
      "('m', '0.0260757')\n",
      "('n', '0.2145354')\n",
      "('o', '0.0005459')\n",
      "('p', '0.0195213')\n",
      "('q', '0.0001749')\n",
      "('r', '0.1104770')\n",
      "('s', '0.0934290')\n",
      "('t', '0.1317960')\n",
      "('u', '0.0098029')\n",
      "('v', '0.0306574')\n",
      "('w', '0.0088799')\n",
      "('x', '0.0009562')\n",
      "('y', '0.0233701')\n",
      "('z', '0.0018701')\n",
      "('.', '0.0715219')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "alphabet = [chr(i+ord('a')) for i in range(26)]\n",
    "alphabet.append('.')\n",
    "letter2idx = {c:i for i,c in enumerate(alphabet)}\n",
    "\n",
    "T = []\n",
    "with open('transitions.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        T.append(row)\n",
    "\n",
    "print('Example')\n",
    "## p(x_t = 'u' | x_{t-1} = 'q')\n",
    "display(Latex(r\"$p(x_t = \\text{'u'} | x_{t-1} = \\text{'q'})$\"))\n",
    "print(T[letter2idx['q']][letter2idx['u']])\n",
    "display(Latex(r\"$p(x_t | x_{t-1} = \\text{'a'})$\"))\n",
    "t = 0.0\n",
    "for c,p in zip(alphabet,T[letter2idx['a']]):\n",
    "    print(c,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    " For a given N, write a program to sample random strings with letters x1, x2, . . . , xN from p(x1:N|x0)\n",
    "\n",
    "For N = 1 it is trivial. We will just need to select the corresponding section of conditional probability table.  Then make a random choice using np.random.choice function.\n",
    "\n",
    "$p(x_1|x_0 = \\text{'.'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t']\n"
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "#convert the string matrix to float. This is one time operation. Otherwise, we cant make calculations.\n",
    "T = [[float(y) for y in x] for x in T]\n",
    "# Assign first column of conditional probability table to a variable called guess_table\n",
    "#letter2idx is key-value pair. It stores the corresponding value for each letter.\n",
    "#Ex: letter2idx['a'] -> 0\n",
    "#  letter2idx['b'] -> 1 , letter2idx['.'] -> 26\n",
    "guess_table = T[letter2idx['.']]\n",
    "print(np.random.choice(alphabet,1,p=guess_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look where N = 2 to illustrate this.\n",
    "\n",
    "$p(x_{1:2} | x_0=\\text{'.'})=p(x_1,x_2|x_0=\\text{'.'})=\\sum_{x_1,x_2}p(x_1|x_0 = \\text{'.'})p(x_2|x_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 2\n",
    "#T[i][j] means p(x_{j}|n_{i})\n",
    "guess_table = np.zeros(shape=(27*27,1))\n",
    "guess_table_margin = np.zeros(shape=(27,1))\n",
    "for i in range(0,len(alphabet)):\n",
    "    for j in range(0,len(alphabet)):\n",
    "        #print(\"x_0= = '.' \" +\"x1=\"+alphabet[i]+\" x2=\"+alphabet[j]+\" \"+ str(np.dot(T[letter2idx['.']][i],T[i][j])))\n",
    "        guess_table[i*27+j] = np.dot(T[i][j],T[letter2idx['.']][i],)\n",
    "        guess_table_margin[i][0] += np.dot(T[i][j],T[letter2idx['.']][i],) \n",
    "#normalize\n",
    "guess_table = np.divide(guess_table,np.sum(guess_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a choice from this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('.', 's')\n"
     ]
    }
   ],
   "source": [
    "number = np.random.choice(range(0,27*27),p = guess_table[:,0])\n",
    "x_1 = np.mod(number,27)\n",
    "x_2 = np.ceil(number/27)\n",
    "print(str(alphabet[x_1]),str(alphabet[int(x_2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We computed random string generation using markov model for N=1 and N=2. However, this array structure is not suitable for bigger N. I did it in this way in the beginning because, I am trying to learn.\n",
    "\n",
    "In the end our output is a table, where the dimensions are 27xN. for x1,x2...xN . We make our selections from that table N times. Let's re formulaze it. In  order achive it we need to marginalize x1, x2 .. Xn\n",
    "\n",
    "In order to check if we approach in correct way. Marginalize over x2 p(x1,x2|x0) to p(x1|xo). Let's see if they are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1062437, 0.0444502, 0.03916, 0.0282947, 0.0213084, 0.0400793, 0.0171783, 0.0606047, 0.0678165, 0.003466, 0.0045451, 0.0243019, 0.0406429, 0.0234882, 0.064992, 0.0273498, 0.0022208, 0.0214068, 0.0704687, 0.1460781, 0.0092399, 0.0079497, 0.0606385, 0.0001107, 0.0114638, 0.0002911, 0.0562102]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(np.divide(guess_table_margin,np.sum(guess_table_margin)))\n",
    "print(T[letter2idx['.']])\n",
    "\n",
    "another_table = np.zeros((27,1))\n",
    "first_table = T[letter2idx['.']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, they are equal.\n",
    "\n",
    "We need to find what is $p(x_{1:N} | x_0)$\n",
    "\n",
    "$p(x_{1:N} | x_0=\\text{'.'})=\\sum_{x_1,x_2,....,X_N}p(x_1|x_0 = \\text{'.'})p(x_2|x_1)p(x_3|x_2)...p(x_{N}|x_{N-1}) $\n",
    "\n",
    "Given x_0 = '.', 27X27 conditional probability table T and number of letters N.\n",
    "\n",
    "Output, guess_table -> 27xN probability table which contains every probability of states x1 x2, ... xn with respect to letters\n",
    "\n",
    "Algorithm should be, \n",
    "\n",
    "i = 0\n",
    "\n",
    "Start with p(x1|x_0 ='.'), which is row vector.  The values are the first column of the guess_table\n",
    "p(x2|x1) is my probability table T. \n",
    "i++\n",
    "\n",
    "multiply every x1 and x2. Marginalize x1.  The values are the second column of guess_table\n",
    "i++\n",
    "multiple every x3 in T and x2 in guess_table. Marginalize on x2 the values are the third column of guess table.\n",
    "i ++\n",
    "\n",
    "return guess_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u n r s e d d o a n . h . z . l w e e d a b . . . i . h a l e . s s c b f t . e e a r . r . e . d r e o t a t a . h f a b b g r d g . p g d m t . i . . i l i h a . . . a l g t e o o h m e t i e . u t . b . r e o h a t a t h . . i e g e k . t n r k c r e n t t i n o e . . f f r y e s s . . i g t . v t . . . d z a i a e r . o o r n a i i a l a d . t . r . e f o i . v . e r t r . h l b f t i a r r h t . s h l a n . n g e n b . e e . . o n . . n r m t g d f i t t r f . u o m c d s e i h h o t b o . p . o r y s c e a r i o r s . e h u i d t t t e p a a h e u y u h l s a d a r r e l f s . s f u l . a o d h t s t s t e e l r r i . . t r e m . . . r . i e i s k g y . . r . . o a o o . . . n c t v e r e e a h e c y . t d t e n a c s i . . e x . m . e n i o h c e o . e l . m t i e n . r a t e a b n . e . r c . w d h o e y . r o . h s s t e h s r w t r d w e . v u h u o u m b n t c g i l f e n e a h t r . . h c e e t . n . . . t a c a n w . u s d s r i . e o a i s l i . d s . o t . l . j . a e l l r l a . h a n e . g . e u h . e . u i i h y . e l d . . . . . e l s e e r e s r a i p n . s u s m a h a b . n l e t d u s h i o o i e e l e o u . s i . . e a t u u n o s e . l u e t o p e y l . l s . h f . . w a e u i a r e o . e x h t n h d . o . c g c . i u a e s a a a . o i y r a r r . s d m . v . t a . n o w n s s i d . o o . e i v . g h a n d w . w c . e o r . i d t p g i o . . o d t d d g s l k h d j n n l e s . i n e . o f h . e m a r t . g e m . n t o o m r . t r o t f r b . s . e e i s i s o n r . . h t n t n w t t t b e o . e p h o . g y t g i c a d o r e y s h w i g v l e i e e c a u a . a s r e i r s b n a . . r e t . m e h d n l m i . . . a r w a u d . i a a a o e o s . g . l . y . a o . a h t n n r i u w h t r c g d h w c b l w r t s . d e w t . i s m a t l a r o t . t a e . h . h r t p h m e s h . r d f t r t l a e s . . t a o a d t . . . r s l l e n n n o i u d o u s . t . n r a . u n . . h e b . a n i l n e . p i f o i l . h t e t g h o e e r n m e h r a r . e . d n\n"
     ]
    }
   ],
   "source": [
    "#27xN shaped matrix\n",
    "# given j  z\n",
    "N = 1000\n",
    "guess_table = np.zeros((27,N+3))\n",
    "x_0 = '.'\n",
    "x_prev = x_0\n",
    "#fill guess_table\n",
    "word = np.zeros((0,1))\n",
    "for i in range(0,len(alphabet)):\n",
    "    guess_table[i,0] = T[letter2idx[x_prev]][i]\n",
    "guess_table[:,0] = np.divide(guess_table[:,0],np.sum(guess_table[:,0]))\n",
    "#print(guess_table[:,0])\n",
    "word = np.vstack((word,str(np.random.choice(alphabet,1,p = guess_table[:,0])[0])))\n",
    "\n",
    "for i in range(1,N):\n",
    "    for j in range(0,27):\n",
    "        for z in range(0,27):\n",
    "            #print(\"x1=\"+alphabet[z]+\" x2=\"+alphabet[j]+\" \"+ str(np.dot(T[z][j],guess_table[z,i-1])))\n",
    "            #compute and marginalize\n",
    "            guess_table[j,i] += np.dot(T[z][j],guess_table[z,i-1])\n",
    "    #normalize\n",
    "    guess_table[:,i] = np.divide(guess_table[:,i],np.sum(guess_table[:,i]))\n",
    "    #make choice\n",
    "    word = np.vstack((word ,str(np.random.choice(alphabet,1,p = guess_table[:,i])[0])))  \n",
    "for i in range(0,len(word)):\n",
    "    print(str(word[i,0])),\n",
    "#guess_table = np.divide(guess_table,np.sum(guess_table))\n",
    "#guess_table[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write the second element of Probability (guess) table as \n",
    "\n",
    "$T^{T}T[letter2idx['.']$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of x2 , product of two distributions computed, then marginalization applied\n",
      "x2= [ 0.06595029  0.00781004  0.01517336  0.01913779  0.13188224  0.01555645\n",
      "  0.00930034  0.07811046  0.06348793  0.00062664  0.00532751  0.03583539\n",
      "  0.01471353  0.05857413  0.07152386  0.01149852  0.00040637  0.04863329\n",
      "  0.04157748  0.05900106  0.03189465  0.00719343  0.00954437  0.000513\n",
      "  0.01585309  0.00067948  0.18019532]\n",
      "The distribution of x2 , product of transpoze of the transition table and p(x1|x0='.') computed \n",
      "x2= [ 0.06595029  0.00781004  0.01517336  0.01913779  0.13188224  0.01555645\n",
      "  0.00930034  0.07811046  0.06348793  0.00062664  0.00532751  0.03583539\n",
      "  0.01471353  0.05857413  0.07152386  0.01149852  0.00040637  0.04863329\n",
      "  0.04157748  0.05900106  0.03189465  0.00719343  0.00954437  0.000513\n",
      "  0.01585309  0.00067948  0.18019532]\n",
      "The result is same, no normalization is applied, however, I will always apply normalization \n"
     ]
    }
   ],
   "source": [
    "print \"The distribution of x2 , product of two distributions computed, then marginalization applied\"\n",
    "print(\"x2=\"),\n",
    "print(guess_table[:,1])\n",
    "print \"The distribution of x2 , product of transpoze of the transition table and p(x1|x0='.') computed \"\n",
    "print(\"x2=\"),\n",
    "print(np.dot(np.transpose(T),T[letter2idx['.']]))\n",
    "print \"The result is same, no normalization is applied, however, I will always apply normalization \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "Question: Now suppose you are given strings with missing letters, where each missing letter is denoted\n",
    "by a question mark (or underscore, as below). Implement a method, that samples missing\n",
    "letters conditioned on observed ones, i.e., samples from p(x−α|xα) where α denotes indices\n",
    "of observed letters. For example, if the input is ’t??.’, we have N = 4 and x1 = ’t’ and\n",
    "x4 = ’.’, α = {1, 4} and −α = {2, 3}. Your program may possibly generate the strings ’the.’,\n",
    "’twi.’, ’tee.’, etc. Hint: make sure to make use all data given and sample from the correct\n",
    "distribution. Implement the method and print the results for the test strings below.\n",
    "\n",
    "We have sucessfully generated our forward markov(1) model. Now let's turn it into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def straight_markov(first_char,N,T,repeat,start,p,l):\n",
    "    #27xN shaped matrix\n",
    "    # given j  z\n",
    "    guess_table = np.zeros((27,N+2))\n",
    "    x_0 = first_char\n",
    "    x_prev = x_0\n",
    "    #fill guess_table\n",
    "    word = np.zeros((0,1))\n",
    "    for i in range(0,len(alphabet)):\n",
    "        guess_table[i,0] = T[letter2idx[x_prev]][i]\n",
    "    guess_table[:,0] = np.divide(guess_table[:,0],np.sum(guess_table[:,0]))\n",
    "    #print(guess_table[:,0])\n",
    "    for i in range(1,N):\n",
    "        for j in range(0,27):\n",
    "            for z in range(0,27):\n",
    "                #print(\"x1=\"+alphabet[z]+\" x2=\"+alphabet[j]+\" \"+ str(np.dot(T[z][j],guess_table[z,i-1])))\n",
    "                #compute and marginalize\n",
    "                guess_table[j,i] += np.dot(T[z][j],guess_table[z,i-1])\n",
    "        #normalize\n",
    "        guess_table[:,i] = np.divide(guess_table[:,i],np.sum(guess_table[:,i]))\n",
    "    for i in range(0,repeat):\n",
    "        word = np.zeros((0,N))\n",
    "        word = np.vstack(first_char)\n",
    "        for j in range(0,N):\n",
    "            if not l:\n",
    "                word = np.vstack((word ,str(np.random.choice(alphabet,1,p = guess_table[:,j])[0]))) \n",
    "            else:\n",
    "                word = np.vstack((word ,str(alphabet[np.argmax(guess_table[:,j])])))\n",
    "        if p :\n",
    "            print(''.join(word[start:,0]))\n",
    "        else:\n",
    "            return word[start:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call our function with x_0 = 't' , N=4, 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttey\n",
      "t.ue\n",
      "t...\n",
      "tian\n",
      "thde\n",
      "thrc\n",
      "teae\n",
      "thmt\n",
      "toab\n",
      "tldh\n"
     ]
    }
   ],
   "source": [
    "straight_markov('t',3,T,10,0,p=True,l=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The markov model is like x0 -> x1 -> x2-> .... xn -> xn+1\n",
    "\n",
    "Let's look at for a _ _ b for getting some intuition. This model is p(x2,x3|x1=a,x4=b). We need to construct our table using these facts.\n",
    "\n",
    "$p(x_2,x_3|x_1=\\text{'a'}, x_4=b)$ $\\alpha$ $p(x_4=\\text{'b'},x_3,x_2,x_1=\\text{'a'}) $\n",
    "\n",
    "$=p(x_4=\\text{'b'}|x_3,x_2,x_1=\\text{'a'})p(x_3,x_2,x_1=\\text{'a'}) $\n",
    "\n",
    "$=p(x_4=\\text{'b'}|x_3,x_2,x_1='a')p(x_3|x_2,x_1='a')p(x_2,x_1='a')$\n",
    "\n",
    "$=p(x_4=\\text{'b'}|x_3,x_2,x_1=\\text{'a'})p(x_3|x_2,x_1=\\text{'a'})p(x_2|x_1=\\text{'a'})p(x_1=\\text{'a'}|x_o = \\text{'.'})$\n",
    "\n",
    "There is also x0 = '.' in this model, since we know that x0 = '.' \n",
    "\n",
    "p(x0 = '.') = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Applying conditional independences the equation turns out to be,\n",
    "\n",
    "$= p(x_4=\\text{'b'}|x_3)p(x_3|x_2)p(x_2|x_1 = \\text{'a'})p(x_1 = \\text{'a'}|x_0 = \\text{'.'})$\n",
    "\n",
    "$p(x_1 = 'a'|x_0 ='.')$ is a scalar.\n",
    "\n",
    "$p(x_2|x_1=\\text{'a'})$ is vector.\n",
    "\n",
    "$p(x_3|x_2)$ is our probability table T.\n",
    "\n",
    "$p(x4=\\text{'b'}|x3)$ is a vector.\n",
    "\n",
    "The result should be a matrix of 27x2, where the row indexes are the elements of domain. Columns are the unkown variables.\n",
    "\n",
    "We need to compute it in a automatic way\n",
    "\n",
    "The inference from future is not as trivial as forward inference.\n",
    "\n",
    "There are two types of string pattern one is a... case. \n",
    "\n",
    "The other is a..b case. There is no such other case only the number of unkowns are changing.\n",
    "\n",
    "I will identify the types of the strings with a function\n",
    "\n",
    "For general case we can prove it with the following:\n",
    "\n",
    "\n",
    "$ p(x_{1:N}|,x_0=\\hat{x_0})$\n",
    "\n",
    "$\\alpha$\n",
    "\n",
    "$p(x_{N+1}=\\hat{x}_{N+1}|x_N)p(x_n|x_{n-1})\\dots p(x_1|x_0=\\hat{x}_0)$\n",
    "\n",
    "The $p(x_1|x_0=\\hat{x}_0)$ term can be obtained easily via row selection.\n",
    "\n",
    "In order to obtain other terms we need to multiply T by N times. \n",
    "\n",
    "Then we need to take the transpoze of T.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########### Inference from Future START ###########  \n",
    "def inference_from_future(first_char,last_char,N,T,start,l):\n",
    "    prob = 1\n",
    "    guess_table = np.zeros((27,N+2))\n",
    "    x_0 = first_char\n",
    "    x_prev = x_0\n",
    "    word = np.zeros((0,1))\n",
    "    #print(\"We have \" + str(N) + \" unkown \")\n",
    "    if(first_char == \"\"):\n",
    "        first_char = '.'\n",
    "    #print(\"x_1\"+ \" = \" + first_char)\n",
    "    #print(\"x_\"+str(N+1)+ \" = \" + last_char)\n",
    "    for i in range(0,N):\n",
    "        matrix = T\n",
    "        for x_d in alphabet:\n",
    "            #print(\"x_\"+str(i + 2)+\" = \" + x_d ),\n",
    "            guess_table[letter2idx[x_d],i] = T[letter2idx[first_char]][letter2idx[x_d]]\n",
    "        else:\n",
    "            for x_d in alphabet:\n",
    "                #print(\"x_\"+str(i + 2)+\" = \" + x_d ),\n",
    "                guess_table[letter2idx[x_d],i] = guess_table[letter2idx[x_d],i]\n",
    "        #for j in range(0,N):\n",
    "        if l:\n",
    "            matrix=np.transpose(np.linalg.matrix_power(T,N-i)) # decrease the number of N since we pretend like we have found the letter\n",
    "        else:\n",
    "            matrix=np.transpose(np.linalg.matrix_power(T,N))\n",
    "        guess_table[:,i] = matrix[letter2idx[last_char]]*guess_table[:,i]\n",
    "        guess_table[:,i] = np.divide(guess_table[:,i],np.sum(guess_table[:,i]))\n",
    "        if l:\n",
    "            first_char = str(alphabet[np.argmax(guess_table[:,i])])\n",
    "            prob = np.dot(prob,guess_table[letter2idx[first_char],i])\n",
    "            # assign argmax as first char.\n",
    "        repeat = 1\n",
    "    for i in range(0,repeat):\n",
    "        word = np.zeros((0,N))\n",
    "        word = np.vstack(first_char)\n",
    "        for j in range(0,N):\n",
    "            if not l:\n",
    "                word = np.vstack((word ,str(np.random.choice(alphabet,1,p = guess_table[:,j])[0]))) \n",
    "            if l:\n",
    "                #print np.argmax(guess_table[:,j])\n",
    "                word = np.vstack((word ,str(alphabet[np.argmax(guess_table[:,j])])))\n",
    "        else:\n",
    "            if not l:\n",
    "                return word[start:]\n",
    "            if l:\n",
    "                return word[start:],prob\n",
    "    \n",
    "########### Inference from Future END ###########  \n",
    "########### STRING OPERATION FUNCTIONS START ###########  \n",
    "def underscore_finder(my_str):\n",
    "    list_of_underscore_indices = []\n",
    "    for i in range(0,len(my_str)):\n",
    "        if(my_str[i] == \"_\"):\n",
    "            list_of_underscore_indices.append(i)\n",
    "    return list_of_underscore_indices\n",
    "def group_problems(list_of_underscore_indices):\n",
    "    problems = []\n",
    "    for i in range(0,len(list_of_underscore_indices)):\n",
    "        problems.append([])\n",
    "    #print(problems)\n",
    "    z = 0\n",
    "    for i in range(0,len(list_of_underscore_indices)):\n",
    "        problems[z].append(list_of_underscore_indices[i])\n",
    "        if(i == len(list_of_underscore_indices)-1):\n",
    "            break\n",
    "        if(list_of_underscore_indices[i + 1] - list_of_underscore_indices[i] !=  1):\n",
    "            z = z + 1\n",
    "    #delete empty lists\n",
    "    return [x for x in problems if x != []]\n",
    "            \n",
    "    \n",
    "def problem_identify(my_str,list_of_underscore_indices,T,l):\n",
    "    str_len = len(my_str)\n",
    "    problems = group_problems(list_of_underscore_indices)\n",
    "    #print(problems)\n",
    "    p_num = len(problems)\n",
    "    pb = 1\n",
    "    #print(\"The amount of problem is : \" + str(p_num))\n",
    "    for i in range(0,p_num):\n",
    "        p_type = 0\n",
    "        #p_type = 1 if straight markov\n",
    "        #p_type = 2 if inference from future is need\n",
    "        #print(\"Working on problem no: \"+ str(i))\n",
    "        problem = problems[i]\n",
    "        if(problem[0] == 0):\n",
    "            prev = \"\"\n",
    "        else:\n",
    "            prev = my_str[problem[0]-1]\n",
    "        if(problem[-1] == (len(my_str) -1) ):\n",
    "            next_letter = \"\"\n",
    "            p_type = 1\n",
    "        else:\n",
    "            next_letter = my_str[problem[-1]+1]\n",
    "            p_type = 2\n",
    "        #print \"type = \", str(p_type),prev,len(problem)*\"_\",next_letter, \" length \",len(problem)\n",
    "        if not l:\n",
    "            my_str = inference(p_type,my_str,problem,prev,next_letter,T,l)\n",
    "        else:\n",
    "            my_str,prob = inference(p_type,my_str,problem,prev,next_letter,T,l)\n",
    "            pb = np.dot(prob,pb)\n",
    "    if l:\n",
    "        print \"Log prob:\", np.log(pb)\n",
    "    return my_str\n",
    "########### STRING OPERATION FUNCTIONS END ########### \n",
    "\n",
    "def inference(p_type,my_str,indices,prev_letter,next_letter,T,l):\n",
    "    prob = 1\n",
    "    if(my_str == \"\"):\n",
    "        #print(\"Null string error\")\n",
    "        return \"\"\n",
    "    if(p_type == 1):\n",
    "        #Straight markov\n",
    "        #print(\"Problem type = 1\")\n",
    "        if prev_letter == \"\":\n",
    "            result = straight_markov(first_char=\".\",T=T,N=len(indices),repeat=1,start=1,p=False,l=l)\n",
    "            #print(\"Straight! 1\")\n",
    "            #print(result)\n",
    "            a = 0\n",
    "            for i in indices:\n",
    "                list1 = list(my_str)\n",
    "                list1[i] = str(\"\".join(result[a]))\n",
    "                a = a+1\n",
    "                my_str = \"\".join(list1)\n",
    "            #print \"Result is \", my_str\n",
    "        else:\n",
    "            result = straight_markov(first_char=prev_letter,T=T,N=len(indices),repeat=1,start=1,p=False,l=l)\n",
    "            #print(\"Straight! 2\")\n",
    "            #print(result)\n",
    "            a = 0\n",
    "            for i in indices:\n",
    "                list1 = list(my_str)\n",
    "                list1[i] = str(\"\".join(result[a]))\n",
    "                a = a+1\n",
    "                my_str = \"\".join(list1)\n",
    "            #print \"Result is \", my_str\n",
    "    elif(p_type == 2):\n",
    "        if l:\n",
    "            result,pb = inference_from_future(first_char = prev_letter,last_char = next_letter,N=len(indices),T=T,start = 1,l=l)\n",
    "            prob = np.dot(pb,prob)\n",
    "        else:\n",
    "            result = inference_from_future(first_char = prev_letter,last_char = next_letter,N=len(indices),T=T,start = 1,l=l)\n",
    "\n",
    "        #print(\"From future!\")\n",
    "        #print(result)\n",
    "        a = 0\n",
    "        for i in indices:\n",
    "            list1 = list(my_str)\n",
    "            list1[i] = str(\"\".join(result[a]))\n",
    "            a = a+1\n",
    "            my_str = \"\".join(list1)\n",
    "        #print \"Result is \", my_str\n",
    "    if l:\n",
    "        return my_str,prob\n",
    "    return my_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples for  th__br__n.f_x.\n",
      "thfebreen.fex.\n",
      "thtebr.kn.fex.\n",
      "tha.bro.n.fex.\n",
      "th..bretn.fex.\n",
      "theebrsin.fex.\n",
      "theebrmon.fex.\n",
      "theobrfkn.fex.\n",
      "theibri.n.fex.\n",
      "thoybrepn.fix.\n",
      "theibr.kn.fex.\n",
      "samples for  _u_st__n_.to_be._nsw_r__\n",
      "fussthhnd.to.be.answar.i\n",
      "sussthind.tombe.onswor.a\n",
      "buesth.nd.to.be.vnswerts\n",
      "ounsteond.tombe.inswir.a\n",
      "buisthhng.tombe.answors.\n",
      "sutsthhno.toube.wnsw.rbs\n",
      "bursthhnd.to.be.answer.r\n",
      "ouesthrnd.toube.answer.e\n",
      "pupst.onk.tombe.wnswarsr\n",
      "busstshne.toube.answer.r\n",
      "samples for  i__at_._a_h_n_._e_r_i_g\n",
      "inkato..athend.hear.ing\n",
      "ittatf.pathend.tecr.ing\n",
      "imnato..athung.beor.ing\n",
      "irsate.hathens.ce.raing\n",
      "iroati.sathane.he.rei.g\n",
      "itnaty.saghind.fe.rcing\n",
      "ielath.wathan..metr.ing\n",
      "iotaty.oatheng.bear.ing\n",
      "iosats.cathend.ceereing\n",
      "inrath.eachend.ve.roing\n",
      "samples for  q___t.___z._____t.__.___.__.\n",
      "quuut.cphz.gdsawt.hp.apl.wr.\n",
      "quuut.amhz.jbda.t.ew.oar..s.\n",
      "quuut.oifz.iaritt.hp..fs.am.\n",
      "quuut.hmsz.aa.wct.fa.sma..b.\n",
      "quuut.bn.z.pd.ect.ss.c.h.tm.\n",
      "quuut.qwaz.patlst.ko.iti.is.\n",
      "quuut.datz.atah.t.th.ey..uo.\n",
      "quuut.hocz.fdomat.ba.tt..ct.\n",
      "quuut.itwz.hcatit.wi.twa.n..\n",
      "quuut.mf.z.ontd.t.ht..co.ao.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test_strings)):\n",
    "    #repeat samples\n",
    "    print \"samples for \" ,test_strings[i]\n",
    "    for j in range(0,10):\n",
    "        print(problem_identify(test_strings[i],underscore_finder(test_strings[i]),T,False))\n",
    "#group_problems(underscore_finder(test_strings[0]))\n",
    "#print(test_strings[1])\n",
    "#problem_identify(test_strings[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part 3\n",
    "Describe a method for filling in the gaps by estimating the most likely letter for each position. Hint: you need to compute\n",
    "$$\n",
    "x_{-\\alpha}^* = \\arg\\max_{x_{-\\alpha}} p(x_{-\\alpha}|x_{\\alpha})\n",
    "$$\n",
    "Implement the method and print the results for the following test strings along with the log-probability  $\\log p(x_{-\\alpha}^*,x_{\\alpha})$.\n",
    "My algorithm is like:\n",
    "\n",
    "Compute the probability for the first unknown element\n",
    "\n",
    "Take the maximum element of the probability distribution (one dimensional). \n",
    "\n",
    "Assign the found element as first element. Decrease the number of multiplication by one. N--\n",
    "\n",
    "Continue until end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log prob: -3.07433486577\n",
      "the.br.an.fex.\n",
      "Log prob: -8.32284514738\n",
      "oursthend.to.be.answere.\n",
      "Log prob: -11.636089996\n",
      "in.ath.wathend.he.r.ing\n",
      "Log prob: -22.9236424228\n",
      "qur.t.thiz.the.at.an.the.an.\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test_strings)):\n",
    "    print(problem_identify(test_strings[i],underscore_finder(test_strings[i]),T,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 4\n",
    "Argmax is a good way to predict most likely sequence of strings. However, I think it is too much constrained, because we dont use other probabilities. If we calculate the density function of probabilities we can have a better understanding of data. \n",
    "\n",
    "For further analysis.We can use factor graph message passing algorithm. That way it will provide better results to this problem. The algorithm will be less complicated. Since it supports more complicated forms of graphs. Thanks to that flexibility we can connect \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
